{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wittgenstein as lw  # RIPPER implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initial Data ===\n",
      "   feature_1  feature_2  feature_3  feature_4  feature_5  target\n",
      "0   0.374540   0.950714   0.731994   0.598658   0.156019       1\n",
      "1   0.155995   0.058084   0.866176   0.601115   0.708073       0\n",
      "2   0.020584   0.969910   0.832443   0.212339   0.181825       0\n",
      "3   0.183405   0.304242   0.524756   0.431945   0.291229       0\n",
      "4   0.611853   0.139494   0.292145   0.366362   0.456070       0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Generation\n",
    "# -------------------------\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 5\n",
    "\n",
    "# Generate random features and a binary target\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "y = np.random.choice([0, 1], size=n_samples)\n",
    "\n",
    "# Create a DataFrame for ease of use\n",
    "feature_names = [f'feature_{i}' for i in range(1, n_features+1)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"=== Initial Data ===\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preprocessed Data ===\n",
      "   feature_1  feature_2  feature_3  feature_4  feature_5  target\n",
      "0  -0.439808   1.425870   0.832798   0.265310  -0.971652       1\n",
      "1  -1.178208  -1.565503   1.290222   0.273730   0.828999       0\n",
      "2  -1.635718   1.490198   1.175225  -1.058834  -0.887479       0\n",
      "3  -1.085598  -0.740579   0.126331  -0.306115  -0.530632       0\n",
      "4   0.362002  -1.292682  -0.666636  -0.530907   0.007034       0\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocessing\n",
    "# -------------------------\n",
    "# For demonstration, we simply standardize the features.\n",
    "scaler = StandardScaler()\n",
    "df[feature_names] = scaler.fit_transform(df[feature_names])\n",
    "\n",
    "print(\"\\n=== Preprocessed Data ===\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train-Test Split ===\n",
      "Training samples: 80, Testing samples: 20\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train-Test Split\n",
    "# -------------------------\n",
    "X = df[feature_names].values\n",
    "y = df['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\n=== Train-Test Split ===\")\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trained RIPPER Model Rules ===\n",
      "[[feature_3=-0.71--0.36] V [feature_2=0.36-0.65]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train RIPPER Model\n",
    "# -------------------------\n",
    "# Convert training data back to DataFrame for RIPPER implementation\n",
    "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "train_df['target'] = y_train  # Add the target column to the DataFrame\n",
    "\n",
    "ripper = lw.RIPPER()\n",
    "ripper.fit(train_df, class_feat='target')  # Just pass the class_feat\n",
    "\n",
    "print(\"\\n=== Trained RIPPER Model Rules ===\")\n",
    "print(ripper.ruleset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optimized Rule Set ===\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Optimize Rule Set\n",
    "# -------------------------\n",
    "# Dummy optimization: Remove rules that are too \"simple\" (for demonstration purposes)\n",
    "def optimize_ruleset(ruleset):\n",
    "    # Convert each rule to string and check if it has more than 5 tokens (words)\n",
    "    optimized_rules = [str(rule) for rule in ruleset if len(str(rule).split()) > 5]\n",
    "    return optimized_rules\n",
    "\n",
    "optimized_ruleset = optimize_ruleset(ripper.ruleset_)\n",
    "print(\"\\n=== Optimized Rule Set ===\")\n",
    "print(optimized_ruleset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Predictions on Test Data ===\n",
      "[True, True, False, False, False, False, False, False, False, False, True, False, False, False, False, True, True, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Compute Predictions\n",
    "# -------------------------\n",
    "# Predict on the test set using the original (unoptimized) model for now\n",
    "y_pred = ripper.predict(pd.DataFrame(X_test, columns=feature_names))\n",
    "print(\"\\n=== Predictions on Test Data ===\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation: Accuracy Score ===\n",
      "Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the Model\n",
    "# -------------------------\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\n=== Evaluation: Accuracy Score ===\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Predictions on New Data ===\n",
      "[False, False, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Predict New Data\n",
    "# -------------------------\n",
    "# Generate new random data (make sure to use the same number of features)\n",
    "new_data = np.random.rand(5, n_features)  # Random values for new data\n",
    "\n",
    "# Preprocess new data with the same scaler, ensuring it has feature names\n",
    "new_data_df = pd.DataFrame(new_data, columns=feature_names)  # Add feature names to new data\n",
    "new_data_scaled = scaler.transform(new_data_df)  # Scale the new data using the same scaler\n",
    "\n",
    "# Predict with the trained RIPPER model\n",
    "new_predictions = ripper.predict(pd.DataFrame(new_data_scaled, columns=feature_names))\n",
    "\n",
    "print(\"\\n=== Predictions on New Data ===\")\n",
    "print(new_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
