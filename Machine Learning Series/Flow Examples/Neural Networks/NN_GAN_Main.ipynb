{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "# ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ----------------------\n",
    "# Preprocessing\n",
    "# ----------------------\n",
    "# Generate random data for faster performance\n",
    "num_samples, input_dim = 1000, 100\n",
    "real_data = torch.randn(num_samples, input_dim)\n",
    "\n",
    "dataset = TensorDataset(real_data)\n",
    "\n",
    "# ----------------------\n",
    "# Train-Test Split\n",
    "# ----------------------\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Base Model\n",
    "# ----------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize models\n",
    "noise_dim = 50\n",
    "gen = Generator(noise_dim, input_dim).to(device)\n",
    "disc = Discriminator(input_dim).to(device)\n",
    "\n",
    "# Optimizers and Loss\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=0.0002)\n",
    "disc_optimizer = optim.Adam(disc.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | D Loss: 1.5105 | G Loss: 0.7729\n",
      "Epoch [2/10] | D Loss: 1.4678 | G Loss: 0.7374\n",
      "Epoch [3/10] | D Loss: 1.4779 | G Loss: 0.7183\n",
      "Epoch [4/10] | D Loss: 1.4301 | G Loss: 0.7314\n",
      "Epoch [5/10] | D Loss: 1.3795 | G Loss: 0.7321\n",
      "Epoch [6/10] | D Loss: 1.3103 | G Loss: 0.7670\n",
      "Epoch [7/10] | D Loss: 1.3025 | G Loss: 0.7813\n",
      "Epoch [8/10] | D Loss: 1.2785 | G Loss: 0.7960\n",
      "Epoch [9/10] | D Loss: 1.2695 | G Loss: 0.7772\n",
      "Epoch [10/10] | D Loss: 1.2443 | G Loss: 0.7580\n"
     ]
    }
   ],
   "source": [
    "# Train Base Model (GAN)\n",
    "# ----------------------\n",
    "def train_gan(epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for real_batch, in train_loader:\n",
    "            real_batch = real_batch.to(device)\n",
    "            batch_size = real_batch.size(0)\n",
    "\n",
    "            # Train Discriminator\n",
    "            disc_optimizer.zero_grad()\n",
    "            real_labels = torch.ones(batch_size, 1, device=device)\n",
    "            fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "\n",
    "            outputs = disc(real_batch)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "\n",
    "            noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "            fake_data = gen(noise)\n",
    "            outputs = disc(fake_data.detach())\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "\n",
    "            disc_loss = real_loss + fake_loss\n",
    "            disc_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            # Train Generator\n",
    "            gen_optimizer.zero_grad()\n",
    "            outputs = disc(fake_data)\n",
    "            gen_loss = criterion(outputs, real_labels)\n",
    "            gen_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {disc_loss.item():.4f} | G Loss: {gen_loss.item():.4f}\")\n",
    "\n",
    "train_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Samples: [[-0.4624232   0.01267088 -0.19819123 -0.01729728  0.14963311 -0.06387186\n",
      "  -0.00896272 -0.03760966 -0.04962032 -0.18005846  0.2499284  -0.02998625\n",
      "  -0.06323873 -0.24105936 -0.15879817 -0.30380717 -0.01485939  0.15918578\n",
      "  -0.2617735  -0.02947399 -0.18233891  0.14906351 -0.20814711 -0.01035118\n",
      "   0.27262878  0.3734521  -0.06116987 -0.16782628  0.1630097  -0.10889757\n",
      "  -0.3791046  -0.2545691  -0.11808609  0.16982019 -0.0085499   0.07322483\n",
      "  -0.33104935  0.04626653  0.30111957  0.2651047   0.46522132  0.23152328\n",
      "  -0.16454524  0.04745779 -0.05885737 -0.16711989  0.39737317 -0.13763535\n",
      "  -0.21468101 -0.17249255 -0.15834104 -0.06591313 -0.30264142 -0.16576162\n",
      "   0.194803    0.08971837 -0.04338685  0.2861731  -0.34678552  0.2688875\n",
      "   0.14566807 -0.05023527  0.32818884  0.26884374 -0.30043936 -0.02828462\n",
      "  -0.22224754  0.08930578 -0.28315994 -0.02926033  0.03947598  0.01411606\n",
      "  -0.05035447  0.23700282  0.55431384 -0.4291202  -0.48415208 -0.06111975\n",
      "   0.10061734  0.10248158 -0.031363    0.37122297 -0.24972944 -0.32598004\n",
      "  -0.13870108  0.3415224   0.34126574  0.19371387 -0.20217492 -0.5026145\n",
      "   0.39439058  0.01015996  0.08685827  0.15570399  0.2658057  -0.03515701\n",
      "   0.05850704 -0.02777575 -0.23337457 -0.28415534]\n",
      " [-0.3566411   0.03738872 -0.592459    0.3931402   0.48091668 -0.30155456\n",
      "  -0.42223084  0.16572385  0.56157064 -0.00227572  0.77247375  0.44747958\n",
      "  -0.02544961 -0.08541334 -0.32798442 -0.07695954 -0.20370679 -0.02596941\n",
      "   0.09228583 -0.14672193 -0.05004975 -0.56718636  0.17985052  0.18448947\n",
      "   0.09210128  0.53462774  0.5698535  -0.53017044  0.1140669  -0.26553163\n",
      "  -0.4059461   0.04327045 -0.18164358  0.37736416  0.12461802  0.02629518\n",
      "  -0.5444537   0.03286095 -0.3321101   0.6819896   0.6025367   0.09397271\n",
      "   0.30527398 -0.09636682  0.3094186  -0.3648288   0.08755957  0.21934967\n",
      "  -0.23476428 -0.5561466   0.10395345  0.11131797 -0.31529212  0.21371937\n",
      "   0.3964891   0.00121401  0.46502882  0.30008146 -0.04075737  0.36811224\n",
      "  -0.4828603  -0.24190822  0.39137903  0.06846034 -0.17518963  0.09729404\n",
      "   0.05195868  0.37282392 -0.44392496 -0.11059844  0.23951755  0.14498731\n",
      "  -0.09972791  0.09642541  0.41827685 -0.1397292  -0.40670615  0.43617448\n",
      "   0.6983102  -0.22067252  0.31279773  0.04417975 -0.35927883 -0.53190625\n",
      "  -0.40870184  0.596921    0.41544375  0.3151208   0.17179139  0.2887962\n",
      "   0.29622427 -0.47037685 -0.05666138  0.3010732   0.39126575 -0.22088526\n",
      "   0.37891397  0.08238952 -0.54061705 -0.22126363]\n",
      " [-0.13741848 -0.43452346 -0.12270606  0.20911646  0.4497354   0.40939707\n",
      "  -0.18432267  0.3478026   0.30622756  0.3437455   0.20629656  0.07055218\n",
      "   0.06303592 -0.37367633 -0.0118473  -0.5269114   0.11083371  0.15443604\n",
      "   0.05778091  0.14946587 -0.62540084  0.01115374  0.11988332 -0.08657214\n",
      "  -0.14883125  0.674262    0.17712489  0.29893482  0.2208684  -0.06363744\n",
      "  -0.553565   -0.411485   -0.33949226  0.04576638  0.29038727  0.1962051\n",
      "  -0.30948788 -0.2954573  -0.04231487 -0.00704776  0.24652748  0.02039534\n",
      "   0.25435826  0.24384744  0.27667406 -0.01896329  0.4506984   0.13091318\n",
      "  -0.59883076  0.05067287 -0.20582691  0.15337332 -0.17383954 -0.28263274\n",
      "   0.33581233  0.18498857  0.06961835  0.19713765 -0.30595952  0.14586335\n",
      "   0.1302455  -0.12815185  0.43590897  0.5431659  -0.04501808  0.3890394\n",
      "  -0.31509092  0.01755367 -0.36488616  0.13253447  0.01666672  0.14320448\n",
      "  -0.2635358   0.06141055  0.50723326 -0.4494743  -0.1444551  -0.0715776\n",
      "   0.19220646  0.21660157  0.03971384  0.01896263 -0.17167702 -0.396607\n",
      "  -0.23984079  0.44788542  0.51423997  0.25269854 -0.08094258 -0.29324728\n",
      "  -0.05752817 -0.24250618 -0.1313508   0.21669243  0.21515356  0.15028575\n",
      "   0.08779813 -0.37023652 -0.26104143 -0.2484163 ]\n",
      " [-0.22577289 -0.5511859  -0.4572807   0.31815463  0.08665819  0.12652537\n",
      "  -0.09217845  0.05817999  0.20083591 -0.04476888  0.2521477   0.16158882\n",
      "  -0.03358832 -0.17404968  0.1358314   0.04778508 -0.0477594   0.2534458\n",
      "   0.1059904   0.05916898 -0.1447982  -0.08800252  0.00283381  0.17916352\n",
      "   0.2792005   0.16936126  0.42392614 -0.33019558  0.4415871  -0.11193114\n",
      "  -0.19163017 -0.20347847 -0.26299733  0.08284683 -0.01786385 -0.00487426\n",
      "  -0.20569487 -0.08005504 -0.04576714  0.13091518  0.42378503  0.1523749\n",
      "  -0.04097486  0.09399531 -0.07772528 -0.09064026 -0.08669359  0.15593842\n",
      "  -0.09753056 -0.02676558  0.01599179  0.02436684  0.05592438  0.08229814\n",
      "   0.48920336  0.3339029   0.01178888  0.21222132 -0.35505345  0.23888086\n",
      "  -0.19025977 -0.01921306  0.34672636 -0.04307112 -0.06586622 -0.16265348\n",
      "   0.06109441  0.26447138 -0.12983602 -0.08346209  0.29494107 -0.1777623\n",
      "   0.18451196  0.06948828  0.01442215 -0.1949616  -0.5167976   0.22393347\n",
      "   0.13978435 -0.12343757 -0.07658602  0.06884778 -0.25475037  0.06413859\n",
      "  -0.27387235  0.36627552  0.4703987   0.3124191   0.02199524 -0.1672873\n",
      "   0.3268289  -0.0783247  -0.1012999   0.32671776  0.4884151   0.0598676\n",
      "   0.18144672 -0.19609839  0.00531646 -0.11388921]\n",
      " [-0.38930553 -0.00210442 -0.64199054 -0.07191809 -0.02194668  0.18950932\n",
      "  -0.115905   -0.00211739  0.36061713 -0.03351399  0.376443    0.09912222\n",
      "  -0.24414709 -0.04516373 -0.08364319  0.23129466  0.07443205  0.41729775\n",
      "   0.07209896  0.16153023 -0.10191687 -0.390872   -0.17181207  0.06262241\n",
      "   0.16912077  0.22160457  0.41426712 -0.13625246 -0.0550032  -0.11149144\n",
      "  -0.3779816  -0.21138334 -0.36310306 -0.15192881  0.28540203  0.20958832\n",
      "  -0.27851912 -0.05180049 -0.03054298  0.22398752  0.55170465  0.0362265\n",
      "   0.12124206  0.00942074 -0.06249253 -0.2089117   0.19445331  0.12223299\n",
      "  -0.42948923 -0.12122172 -0.03079155 -0.06438739  0.07481476  0.13212585\n",
      "   0.39946997  0.54745215  0.35692245  0.38928097 -0.08281717  0.46236834\n",
      "   0.03877593  0.05531405  0.26809713  0.11862599  0.28219095 -0.16342697\n",
      "   0.07212309  0.29046497 -0.28445902  0.01094736  0.5303758  -0.0637188\n",
      "   0.51171553  0.06279817  0.32487446 -0.26973096 -0.22491705  0.57172114\n",
      "   0.13983881 -0.3092966   0.26578674 -0.0707331  -0.42830655 -0.38396445\n",
      "  -0.06210622  0.12046551  0.14393109  0.16902906  0.05569886 -0.07930331\n",
      "   0.5666797  -0.16669546 -0.14543411  0.34081227  0.49832582 -0.2680346\n",
      "   0.02144386 -0.00479245 -0.14714155 -0.19040298]]\n",
      "Epoch [1/5] | D Loss: 1.2319 | G Loss: 0.7800\n",
      "Epoch [2/5] | D Loss: 1.1603 | G Loss: 0.7748\n",
      "Epoch [3/5] | D Loss: 1.2214 | G Loss: 0.7677\n",
      "Epoch [4/5] | D Loss: 1.1755 | G Loss: 0.7563\n",
      "Epoch [5/5] | D Loss: 1.1570 | G Loss: 0.7272\n",
      "Discriminator Accuracy on Real Data: 89.00%\n",
      "Generator model saved for deployment.\n"
     ]
    }
   ],
   "source": [
    "# Planning (Simulated Experience Using GAN-Generated Samples)\n",
    "# ----------------------\n",
    "def simulate_experience(samples=5):\n",
    "    noise = torch.randn(samples, noise_dim, device=device)\n",
    "    generated_samples = gen(noise)\n",
    "    print(\"Simulated Samples:\", generated_samples.cpu().detach().numpy())\n",
    "\n",
    "simulate_experience()\n",
    "\n",
    "# ----------------------\n",
    "# Fine-Tune Model\n",
    "# ----------------------\n",
    "def fine_tune_model(epochs=5, new_lr=0.0001):\n",
    "    for param_group in gen_optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "    \n",
    "    train_gan(epochs=epochs)\n",
    "\n",
    "fine_tune_model()\n",
    "\n",
    "# ----------------------\n",
    "# Evaluate\n",
    "# ----------------------\n",
    "def evaluate_model():\n",
    "    disc.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for real_batch, in test_loader:\n",
    "            real_batch = real_batch.to(device)\n",
    "            outputs = disc(real_batch)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == 1).sum().item()\n",
    "            total += real_batch.size(0)\n",
    "    \n",
    "    print(f\"Discriminator Accuracy on Real Data: {correct / total * 100:.2f}%\")\n",
    "\n",
    "evaluate_model()\n",
    "\n",
    "# ----------------------\n",
    "# Deploy Policy (Placeholder)\n",
    "# ----------------------\n",
    "def deploy_policy():\n",
    "    torch.save(gen.state_dict(), \"generator_model.pth\")\n",
    "    print(\"Generator model saved for deployment.\")\n",
    "\n",
    "deploy_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
