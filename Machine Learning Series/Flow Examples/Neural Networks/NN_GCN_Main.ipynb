{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Environment Setup\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 2. Preprocessing - Generate random graph and features\n",
    "num_nodes = 100\n",
    "num_features = 16\n",
    "num_classes = 3\n",
    "\n",
    "G = nx.erdos_renyi_graph(num_nodes, 0.1)\n",
    "edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()\n",
    "x = torch.randn((num_nodes, num_features))\n",
    "y = torch.randint(0, num_classes, (num_nodes,))\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "train_mask, test_mask = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42)\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train_mask] = True\n",
    "data.test_mask[test_mask] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 100 nodes and 529 edges.\n",
      "Average node degree: 10.58\n",
      "Epoch 1, Loss: 1.1699\n",
      "Epoch 2, Loss: 1.0888\n",
      "Epoch 3, Loss: 1.0625\n",
      "Epoch 4, Loss: 1.0525\n",
      "Epoch 5, Loss: 1.0351\n",
      "Epoch 6, Loss: 1.0124\n",
      "Epoch 7, Loss: 0.9918\n",
      "Epoch 8, Loss: 0.9774\n",
      "Epoch 9, Loss: 0.9689\n",
      "Epoch 10, Loss: 0.9628\n",
      "Embedding shape: torch.Size([100, 32])\n",
      "Fine-Tuning Epoch 1, Loss: 0.9554\n",
      "Fine-Tuning Epoch 2, Loss: 0.9505\n",
      "Fine-Tuning Epoch 3, Loss: 0.9445\n",
      "Fine-Tuning Epoch 4, Loss: 0.9381\n",
      "Fine-Tuning Epoch 5, Loss: 0.9320\n",
      "Test Accuracy: 0.2500\n",
      "Model ready for deployment.\n"
     ]
    }
   ],
   "source": [
    "# 4. Train Base Model (GCN)\n",
    "model = GCN(in_channels=num_features, hidden_channels=32, out_channels=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# 5. Planning (Graph Structure Exploration & Embedding Analysis)\n",
    "def explore_graph():\n",
    "    print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "    degrees = [deg for _, deg in G.degree()]\n",
    "    print(f\"Average node degree: {np.mean(degrees):.2f}\")\n",
    "\n",
    "def analyze_embeddings():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.conv1(data.x, data.edge_index)\n",
    "    print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# 6. Fine-Tune Model\n",
    "def fine_tune():\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.005  # Adjust learning rate\n",
    "\n",
    "# 7. Evaluate\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out[data.test_mask].argmax(dim=1)\n",
    "        acc = (pred == data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# 8. Deploy Policy (Mock Deployment)\n",
    "def deploy_policy():\n",
    "    print(\"Model ready for deployment.\")\n",
    "\n",
    "# Workflow Execution\n",
    "explore_graph()\n",
    "for epoch in range(10):\n",
    "    loss = train()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "analyze_embeddings()\n",
    "fine_tune()\n",
    "for epoch in range(5):\n",
    "    loss = train()\n",
    "    print(f\"Fine-Tuning Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "\n",
    "evaluate()\n",
    "deploy_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
