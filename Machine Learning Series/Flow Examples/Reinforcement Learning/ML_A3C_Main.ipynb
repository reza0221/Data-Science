{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized training...\n",
      "Ep 1 | Avg Reward: 2.6 | Last: 26.0\n",
      "Ep 2 | Avg Reward: 4.0 | Last: 14.0\n",
      "Ep 3 | Avg Reward: 5.6 | Last: 16.0\n",
      "Ep 4 | Avg Reward: 8.0 | Last: 24.0\n",
      "Ep 5 | Avg Reward: 10.2 | Last: 22.0\n",
      "Ep 6 | Avg Reward: 12.2 | Last: 20.0\n",
      "Ep 7 | Avg Reward: 13.5 | Last: 13.0\n",
      "Ep 8 | Avg Reward: 15.7 | Last: 22.0\n",
      "Ep 9 | Avg Reward: 17.1 | Last: 14.0\n",
      "Ep 10 | Avg Reward: 18.7 | Last: 16.0\n",
      "Ep 11 | Avg Reward: 17.9 | Last: 18.0\n",
      "Ep 12 | Avg Reward: 17.6 | Last: 11.0\n",
      "Ep 13 | Avg Reward: 17.2 | Last: 12.0\n",
      "Ep 14 | Avg Reward: 16.0 | Last: 12.0\n",
      "Ep 15 | Avg Reward: 15.4 | Last: 16.0\n",
      "Ep 16 | Avg Reward: 15.4 | Last: 20.0\n",
      "Ep 17 | Avg Reward: 15.9 | Last: 18.0\n",
      "Ep 18 | Avg Reward: 15.3 | Last: 16.0\n",
      "Ep 19 | Avg Reward: 15.1 | Last: 12.0\n",
      "Ep 20 | Avg Reward: 14.7 | Last: 12.0\n",
      "Ep 21 | Avg Reward: 14.0 | Last: 11.0\n",
      "Ep 22 | Avg Reward: 14.1 | Last: 12.0\n",
      "Ep 23 | Avg Reward: 13.8 | Last: 9.0\n",
      "Ep 24 | Avg Reward: 13.8 | Last: 12.0\n",
      "Ep 25 | Avg Reward: 14.0 | Last: 18.0\n",
      "Ep 26 | Avg Reward: 13.4 | Last: 14.0\n",
      "Ep 27 | Avg Reward: 13.5 | Last: 19.0\n",
      "Ep 28 | Avg Reward: 13.4 | Last: 15.0\n",
      "Ep 29 | Avg Reward: 13.2 | Last: 10.0\n",
      "Ep 30 | Avg Reward: 13.4 | Last: 14.0\n",
      "Ep 31 | Avg Reward: 14.1 | Last: 18.0\n",
      "Ep 32 | Avg Reward: 14.6 | Last: 17.0\n",
      "Ep 33 | Avg Reward: 17.0 | Last: 33.0\n",
      "Ep 34 | Avg Reward: 19.3 | Last: 35.0\n",
      "Ep 35 | Avg Reward: 18.8 | Last: 13.0\n",
      "Ep 36 | Avg Reward: 19.2 | Last: 18.0\n",
      "Ep 37 | Avg Reward: 18.5 | Last: 12.0\n",
      "Ep 38 | Avg Reward: 20.3 | Last: 33.0\n",
      "Ep 39 | Avg Reward: 23.5 | Last: 42.0\n",
      "Ep 40 | Avg Reward: 24.1 | Last: 20.0\n",
      "Ep 41 | Avg Reward: 24.5 | Last: 22.0\n",
      "Ep 42 | Avg Reward: 28.2 | Last: 54.0\n",
      "Ep 43 | Avg Reward: 33.3 | Last: 84.0\n",
      "Ep 44 | Avg Reward: 31.8 | Last: 20.0\n",
      "Ep 45 | Avg Reward: 31.8 | Last: 13.0\n",
      "Ep 46 | Avg Reward: 32.9 | Last: 29.0\n",
      "Ep 47 | Avg Reward: 33.6 | Last: 19.0\n",
      "Ep 48 | Avg Reward: 32.3 | Last: 20.0\n",
      "Ep 49 | Avg Reward: 30.4 | Last: 23.0\n",
      "Ep 50 | Avg Reward: 29.9 | Last: 15.0\n",
      "\n",
      "Final evaluation:\n",
      "Evaluation Average: 17.7\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.002  # Increased learning rate for faster convergence\n",
    "NUM_WORKERS = 1         # Reduced parallel processes\n",
    "HIDDEN_DIM = 32         # Smaller network\n",
    "MAX_EPISODES = 50       # More episodes with simpler computation\n",
    "MAX_STEPS = 200         # Increased steps per episode\n",
    "GRAD_CLIP = 1.0         # More aggressive gradient clipping\n",
    "# =============================================================\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class LightweightActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # Simplified network architecture\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        policy = torch.softmax(self.actor(x), dim=-1)\n",
    "        value = self.critic(x).squeeze(-1)\n",
    "        return policy, value\n",
    "\n",
    "def optimized_worker(env):\n",
    "    state, _ = env.reset()\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "            policy, _ = model(state_tensor.unsqueeze(0))\n",
    "            action = Categorical(policy).sample().item()\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        yield (state, action, reward, next_state, done)\n",
    "        \n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "def efficient_training(global_model):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    optimizer = optim.RMSprop(global_model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    state_batch = []\n",
    "    action_batch = []\n",
    "    reward_batch = []\n",
    "\n",
    "    worker_gen = optimized_worker(env)\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        total_reward = 0\n",
    "        for step in range(MAX_STEPS):\n",
    "            state, action, reward, next_state, done = next(worker_gen)\n",
    "            \n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done or (step == MAX_STEPS - 1):\n",
    "                # Batch processing for efficiency\n",
    "                states = torch.FloatTensor(state_batch).to(device)\n",
    "                actions = torch.LongTensor(action_batch).to(device)\n",
    "                rewards = torch.FloatTensor(reward_batch).to(device)\n",
    "\n",
    "                # Calculate returns\n",
    "                returns = torch.zeros_like(rewards)\n",
    "                running_return = 0\n",
    "                for i in reversed(range(len(rewards))):\n",
    "                    running_return = rewards[i] + GAMMA * running_return\n",
    "                    returns[i] = running_return\n",
    "\n",
    "                # Normalize returns\n",
    "                returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "                # Forward pass\n",
    "                policies, values = global_model(states)\n",
    "                dist = Categorical(policies)\n",
    "                log_probs = dist.log_prob(actions)\n",
    "                \n",
    "                # Calculate losses\n",
    "                advantage = returns - values.detach()\n",
    "                actor_loss = -(log_probs * advantage).mean()\n",
    "                critic_loss = advantage.pow(2).mean()\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "                # Optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(global_model.parameters(), GRAD_CLIP)\n",
    "                optimizer.step()\n",
    "\n",
    "                # Reset batches\n",
    "                state_batch.clear()\n",
    "                action_batch.clear()\n",
    "                reward_batch.clear()\n",
    "\n",
    "                episode_rewards.append(total_reward)\n",
    "                print(f\"Ep {episode+1} | Avg Reward: {sum(episode_rewards[-10:])/10:.1f} | Last: {total_reward}\")\n",
    "                break\n",
    "\n",
    "def quick_evaluate(model, episodes=3):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    total_reward = 0\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).to(device)\n",
    "                policy, _ = model(state_tensor.unsqueeze(0))\n",
    "                action = policy.argmax().item()\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "        total_reward += episode_reward\n",
    "    print(f\"Evaluation Average: {total_reward/episodes:.1f}\")\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    input_dim, action_dim = env.observation_space.shape[0], env.action_space.n\n",
    "    env.close()\n",
    "\n",
    "    model = LightweightActorCritic(input_dim, action_dim).to(device)\n",
    "    \n",
    "    print(\"Starting optimized training...\")\n",
    "    efficient_training(model)\n",
    "    \n",
    "    print(\"\\nFinal evaluation:\")\n",
    "    quick_evaluate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
