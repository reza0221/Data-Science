{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Base Model (DDPG)...\n",
      "Episode 1/80, Reward: -1193.70\n",
      "Episode 2/80, Reward: -1571.87\n",
      "Episode 3/80, Reward: -1360.30\n",
      "Episode 4/80, Reward: -1100.12\n",
      "Episode 5/80, Reward: -1489.19\n",
      "Episode 6/80, Reward: -1806.54\n",
      "Episode 7/80, Reward: -1476.07\n",
      "Episode 8/80, Reward: -1416.23\n",
      "Episode 9/80, Reward: -1550.52\n",
      "Episode 10/80, Reward: -1285.29\n",
      "Episode 11/80, Reward: -1537.18\n",
      "Episode 12/80, Reward: -1464.88\n",
      "Episode 13/80, Reward: -1502.67\n",
      "Episode 14/80, Reward: -1087.02\n",
      "Episode 15/80, Reward: -1398.54\n",
      "Episode 16/80, Reward: -1036.15\n",
      "Episode 17/80, Reward: -1052.90\n",
      "Episode 18/80, Reward: -905.35\n",
      "Episode 19/80, Reward: -753.82\n",
      "Episode 20/80, Reward: -758.40\n",
      "Episode 21/80, Reward: -1058.36\n",
      "Episode 22/80, Reward: -1049.74\n",
      "Episode 23/80, Reward: -885.84\n",
      "Episode 24/80, Reward: -712.39\n",
      "Episode 25/80, Reward: -633.58\n",
      "Episode 26/80, Reward: -555.69\n",
      "Episode 27/80, Reward: -631.05\n",
      "Episode 28/80, Reward: -513.95\n",
      "Episode 29/80, Reward: -247.84\n",
      "Episode 30/80, Reward: -261.23\n",
      "Episode 31/80, Reward: -134.97\n",
      "Episode 32/80, Reward: -374.08\n",
      "Episode 33/80, Reward: -232.30\n",
      "Episode 34/80, Reward: -116.96\n",
      "Episode 35/80, Reward: -267.12\n",
      "Episode 36/80, Reward: -253.83\n",
      "Episode 37/80, Reward: -124.37\n",
      "Episode 38/80, Reward: -3.90\n",
      "Episode 39/80, Reward: -129.33\n",
      "Episode 40/80, Reward: -7.02\n",
      "Episode 41/80, Reward: -130.68\n",
      "Episode 42/80, Reward: -0.55\n",
      "Episode 43/80, Reward: -255.63\n",
      "Episode 44/80, Reward: -251.86\n",
      "Episode 45/80, Reward: -125.78\n",
      "Episode 46/80, Reward: -1.14\n",
      "Episode 47/80, Reward: -241.52\n",
      "Episode 48/80, Reward: -119.10\n",
      "Episode 49/80, Reward: -312.03\n",
      "Episode 50/80, Reward: -377.63\n",
      "Episode 51/80, Reward: -129.39\n",
      "Episode 52/80, Reward: -124.20\n",
      "Episode 53/80, Reward: -117.60\n",
      "Episode 54/80, Reward: -121.11\n",
      "Episode 55/80, Reward: -242.46\n",
      "Episode 56/80, Reward: -237.30\n",
      "Episode 57/80, Reward: -128.48\n",
      "Episode 58/80, Reward: -3.81\n",
      "Episode 59/80, Reward: -230.71\n",
      "Episode 60/80, Reward: -291.79\n",
      "Episode 61/80, Reward: -125.72\n",
      "Episode 62/80, Reward: -116.81\n",
      "Episode 63/80, Reward: -2.71\n",
      "Episode 64/80, Reward: -235.20\n",
      "Episode 65/80, Reward: -122.56\n",
      "Episode 66/80, Reward: -126.71\n",
      "Episode 67/80, Reward: -122.44\n",
      "Episode 68/80, Reward: -281.70\n",
      "Episode 69/80, Reward: -119.03\n",
      "Episode 70/80, Reward: -118.14\n",
      "Episode 71/80, Reward: -117.08\n",
      "Episode 72/80, Reward: -346.39\n",
      "Episode 73/80, Reward: -2.98\n",
      "Episode 74/80, Reward: -120.11\n",
      "Episode 75/80, Reward: -308.95\n",
      "Episode 76/80, Reward: -121.17\n",
      "Episode 77/80, Reward: -263.67\n",
      "Episode 78/80, Reward: -128.05\n",
      "Episode 79/80, Reward: -119.75\n",
      "Episode 80/80, Reward: -2.57\n",
      "\n",
      "Fine-tuning the model...\n",
      "Episode 1/5, Reward: -233.78\n",
      "Episode 2/5, Reward: -127.47\n",
      "Episode 3/5, Reward: -230.95\n",
      "Episode 4/5, Reward: -235.36\n",
      "Episode 5/5, Reward: -242.99\n",
      "Test Episode 1/10, Reward: -248.39\n",
      "Test Episode 2/10, Reward: -119.41\n",
      "Test Episode 3/10, Reward: -124.19\n",
      "Test Episode 4/10, Reward: -120.70\n",
      "Test Episode 5/10, Reward: -127.49\n",
      "Test Episode 6/10, Reward: -236.26\n",
      "Test Episode 7/10, Reward: -118.10\n",
      "Test Episode 8/10, Reward: -127.51\n",
      "Test Episode 9/10, Reward: -118.35\n",
      "Test Episode 10/10, Reward: -125.76\n",
      "\n",
      "Average Test Reward: -146.62\n",
      "\n",
      "Deploying trained policy...\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('Pendulum-v1', render_mode=\"rgb_array\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = float(env.action_space.high[0])\n",
    "\n",
    "# Preprocessing (No heavy preprocessing for this lightweight setup)\n",
    "def preprocess_state(state):\n",
    "    return torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "# Train-Test Split\n",
    "train_episodes = 80\n",
    "val_episodes = 10\n",
    "test_episodes = 10\n",
    "\n",
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.out = nn.Linear(64, action_dim)\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.out(x)) * self.action_bound\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (\n",
    "            torch.stack(states),\n",
    "            torch.stack(actions),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "            torch.stack(next_states),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize networks and optimizers\n",
    "actor = Actor(state_dim, action_dim, action_bound)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "actor_target = Actor(state_dim, action_dim, action_bound)\n",
    "critic_target = Critic(state_dim, action_dim)\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.002)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "def soft_update(target, source, tau=0.005):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(tau * source_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "def get_action(state, noise_scale=0.1):\n",
    "    state = preprocess_state(state).unsqueeze(0)\n",
    "    action = actor(state).detach().numpy()[0]\n",
    "    noise = noise_scale * np.random.randn(action_dim)\n",
    "    return np.clip(action + noise, -action_bound, action_bound)\n",
    "\n",
    "# Train Base Model (DDPG)\n",
    "def train_ddpg(episodes):\n",
    "    gamma = 0.99\n",
    "    batch_size = 64\n",
    "    min_buffer_size = 500\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        for _ in range(200):\n",
    "            action = get_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            replay_buffer.push(preprocess_state(state), torch.tensor(action, dtype=torch.float32), reward, preprocess_state(next_state), done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(replay_buffer) >= min_buffer_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Critic loss\n",
    "                with torch.no_grad():\n",
    "                    target_actions = actor_target(next_states)\n",
    "                    target_q = critic_target(next_states, target_actions)\n",
    "                    y = rewards + gamma * (1 - dones) * target_q\n",
    "                critic_loss = nn.MSELoss()(critic(states, actions), y)\n",
    "\n",
    "                critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                critic_optimizer.step()\n",
    "\n",
    "                # Actor loss\n",
    "                actor_loss = -critic(states, actor(states)).mean()\n",
    "\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # Soft update\n",
    "                soft_update(actor_target, actor)\n",
    "                soft_update(critic_target, critic)\n",
    "\n",
    "        print(f\"Episode {episode+1}/{episodes}, Reward: {episode_reward:.2f}\")\n",
    "\n",
    "# Fine-Tune Model\n",
    "def fine_tune(episodes):\n",
    "    print(\"\\nFine-tuning the model...\")\n",
    "    train_ddpg(episodes)\n",
    "\n",
    "# Evaluate\n",
    "def evaluate(episodes):\n",
    "    total_reward = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        for _ in range(200):\n",
    "            action = actor(preprocess_state(state).unsqueeze(0)).detach().numpy()[0]\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Test Episode {episode+1}/{episodes}, Reward: {episode_reward:.2f}\")\n",
    "    avg_reward = total_reward / episodes\n",
    "    print(f\"\\nAverage Test Reward: {avg_reward:.2f}\")\n",
    "\n",
    "# Deploy Policy (Simulate Deployment)\n",
    "def deploy_policy():\n",
    "    print(\"\\nDeploying trained policy...\")\n",
    "    state = env.reset()[0]\n",
    "    for _ in range(200):\n",
    "        env.render()\n",
    "        action = actor(preprocess_state(state).unsqueeze(0)).detach().numpy()[0]\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "# Main Workflow\n",
    "print(\"Training Base Model (DDPG)...\")\n",
    "train_ddpg(train_episodes)\n",
    "\n",
    "fine_tune(5)  # Fine-tune for 5 episodes\n",
    "\n",
    "evaluate(test_episodes)\n",
    "\n",
    "deploy_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
