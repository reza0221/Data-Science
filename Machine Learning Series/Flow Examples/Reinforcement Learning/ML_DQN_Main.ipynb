{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network for DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPSILON = 1.0  # Exploration rate\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "MEMORY_SIZE = 1000\n",
    "EPISODES = 100\n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "policy_net = DQN(state_size, action_size)\n",
    "target_net = DQN(state_size, action_size)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Experience Replay Function\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def act(state):\n",
    "    global EPSILON\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return random.randrange(action_size)\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        return torch.argmax(policy_net(state)).item()\n",
    "\n",
    "def replay():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    states = torch.FloatTensor(np.array(states))\n",
    "    actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    dones = torch.FloatTensor(dones)\n",
    "    \n",
    "    # Compute target Q-values\n",
    "    next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "    target_q_values = rewards + (GAMMA * next_q_values * (1 - dones))\n",
    "    \n",
    "    # Compute current Q-values\n",
    "    q_values = policy_net(states).gather(1, actions).squeeze(1)\n",
    "    \n",
    "    # Loss and backpropagation\n",
    "    loss = criterion(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 21.0\n",
      "Episode 2: Reward = 34.0\n",
      "Episode 3: Reward = 32.0\n",
      "Episode 4: Reward = 9.0\n",
      "Episode 5: Reward = 11.0\n",
      "Episode 6: Reward = 12.0\n",
      "Episode 7: Reward = 16.0\n",
      "Episode 8: Reward = 28.0\n",
      "Episode 9: Reward = 15.0\n",
      "Episode 10: Reward = 26.0\n",
      "Episode 11: Reward = 23.0\n",
      "Episode 12: Reward = 17.0\n",
      "Episode 13: Reward = 11.0\n",
      "Episode 14: Reward = 11.0\n",
      "Episode 15: Reward = 11.0\n",
      "Episode 16: Reward = 14.0\n",
      "Episode 17: Reward = 11.0\n",
      "Episode 18: Reward = 9.0\n",
      "Episode 19: Reward = 13.0\n",
      "Episode 20: Reward = 10.0\n",
      "Episode 21: Reward = 14.0\n",
      "Episode 22: Reward = 13.0\n",
      "Episode 23: Reward = 17.0\n",
      "Episode 24: Reward = 43.0\n",
      "Episode 25: Reward = 9.0\n",
      "Episode 26: Reward = 11.0\n",
      "Episode 27: Reward = 12.0\n",
      "Episode 28: Reward = 9.0\n",
      "Episode 29: Reward = 10.0\n",
      "Episode 30: Reward = 9.0\n",
      "Episode 31: Reward = 10.0\n",
      "Episode 32: Reward = 13.0\n",
      "Episode 33: Reward = 18.0\n",
      "Episode 34: Reward = 10.0\n",
      "Episode 35: Reward = 21.0\n",
      "Episode 36: Reward = 34.0\n",
      "Episode 37: Reward = 19.0\n",
      "Episode 38: Reward = 29.0\n",
      "Episode 39: Reward = 35.0\n",
      "Episode 40: Reward = 13.0\n",
      "Episode 41: Reward = 10.0\n",
      "Episode 42: Reward = 11.0\n",
      "Episode 43: Reward = 42.0\n",
      "Episode 44: Reward = 62.0\n",
      "Episode 45: Reward = 22.0\n",
      "Episode 46: Reward = 22.0\n",
      "Episode 47: Reward = 32.0\n",
      "Episode 48: Reward = 79.0\n",
      "Episode 49: Reward = 61.0\n",
      "Episode 50: Reward = 69.0\n",
      "Episode 51: Reward = 14.0\n",
      "Episode 52: Reward = 20.0\n",
      "Episode 53: Reward = 10.0\n",
      "Episode 54: Reward = 17.0\n",
      "Episode 55: Reward = 16.0\n",
      "Episode 56: Reward = 27.0\n",
      "Episode 57: Reward = 21.0\n",
      "Episode 58: Reward = 31.0\n",
      "Episode 59: Reward = 12.0\n",
      "Episode 60: Reward = 59.0\n",
      "Episode 61: Reward = 29.0\n",
      "Episode 62: Reward = 51.0\n",
      "Episode 63: Reward = 42.0\n",
      "Episode 64: Reward = 23.0\n",
      "Episode 65: Reward = 39.0\n",
      "Episode 66: Reward = 54.0\n",
      "Episode 67: Reward = 72.0\n",
      "Episode 68: Reward = 113.0\n",
      "Episode 69: Reward = 131.0\n",
      "Episode 70: Reward = 40.0\n",
      "Episode 71: Reward = 73.0\n",
      "Episode 72: Reward = 62.0\n",
      "Episode 73: Reward = 57.0\n",
      "Episode 74: Reward = 30.0\n",
      "Episode 75: Reward = 26.0\n",
      "Episode 76: Reward = 72.0\n",
      "Episode 77: Reward = 60.0\n",
      "Episode 78: Reward = 205.0\n",
      "Episode 79: Reward = 27.0\n",
      "Episode 80: Reward = 87.0\n",
      "Episode 81: Reward = 32.0\n",
      "Episode 82: Reward = 125.0\n",
      "Episode 83: Reward = 123.0\n",
      "Episode 84: Reward = 57.0\n",
      "Episode 85: Reward = 114.0\n",
      "Episode 86: Reward = 119.0\n",
      "Episode 87: Reward = 88.0\n",
      "Episode 88: Reward = 101.0\n",
      "Episode 89: Reward = 75.0\n",
      "Episode 90: Reward = 73.0\n",
      "Episode 91: Reward = 76.0\n",
      "Episode 92: Reward = 45.0\n",
      "Episode 93: Reward = 50.0\n",
      "Episode 94: Reward = 11.0\n",
      "Episode 95: Reward = 113.0\n",
      "Episode 96: Reward = 106.0\n",
      "Episode 97: Reward = 104.0\n",
      "Episode 98: Reward = 35.0\n",
      "Episode 99: Reward = 125.0\n",
      "Episode 100: Reward = 115.0\n"
     ]
    }
   ],
   "source": [
    "# Train DQN Model\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        replay()\n",
    "    \n",
    "    # Decay epsilon\n",
    "    if EPSILON > EPSILON_MIN:\n",
    "        EPSILON *= EPSILON_DECAY\n",
    "    \n",
    "    # Update Target Network Periodically\n",
    "    if episode % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: Reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved and ready for deployment!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained policy model\n",
    "torch.save(policy_net.state_dict(), \"dqn_cartpole.pth\")\n",
    "print(\"Model saved and ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
