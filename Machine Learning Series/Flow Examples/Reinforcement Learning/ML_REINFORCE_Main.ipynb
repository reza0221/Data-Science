{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Environment\n",
    "class DummyEnv:\n",
    "    def __init__(self):\n",
    "        self.state_space = 4  # Example state dimension\n",
    "        self.action_space = 2  # Example action dimension\n",
    "\n",
    "    def reset(self):\n",
    "        return np.random.rand(self.state_space)  # Random initial state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate environment response with random reward\n",
    "        next_state = np.random.rand(self.state_space)\n",
    "        reward = np.random.randn()\n",
    "        done = np.random.rand() > 0.95  # Randomly terminate episodes\n",
    "        return next_state, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocessing (simulate with random data)\n",
    "def preprocess_data(data):\n",
    "    std = np.std(data, axis=0) + 1e-9  # Prevent division by zero\n",
    "    return (data - np.mean(data, axis=0)) / std\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "def train_test_split(data, split_ratio=0.8):\n",
    "    split_idx = int(len(data) * split_ratio)\n",
    "    return data[:split_idx], data[split_idx:]\n",
    "\n",
    "# Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=4, output_dim=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 16)\n",
    "        self.output = nn.Linear(16, output_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc(state))\n",
    "        action_probs = F.softmax(self.output(x), dim=-1)  # Ensure valid probability distribution\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env, policy, optimizer, num_episodes=100, gamma=0.99):\n",
    "    for episode in range(num_episodes):\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action_probs = policy(state)\n",
    "            distribution = Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "\n",
    "            log_probs.append(distribution.log_prob(action))\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "        # Compute returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "        # âœ… Only normalize if returns has more than 1 value\n",
    "        if len(returns) > 1:\n",
    "            std_val = returns.std(unbiased=False)\n",
    "            if std_val > 1e-8:  # Avoid division by zero\n",
    "                returns = (returns - returns.mean()) / std_val\n",
    "\n",
    "        # Compute loss\n",
    "        loss = -torch.sum(torch.stack(log_probs) * returns)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode + 1:03d} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    total_reward = 0\n",
    "    for _ in range(num_episodes):\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action_probs = policy(state)\n",
    "            action = torch.argmax(action_probs).item()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        total_reward += episode_reward\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {total_reward / num_episodes:.2f}\")\n",
    "\n",
    "# Dummy Deployment\n",
    "def deploy_policy(policy):\n",
    "    print(\"Policy deployed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: 80 training samples, 20 testing samples.\n",
      "\n",
      "Training policy using REINFORCE...\n",
      "\n",
      "Episode 001 | Loss: 1.6313\n",
      "Episode 002 | Loss: -0.6457\n",
      "Episode 003 | Loss: -0.6906\n",
      "Episode 004 | Loss: 0.4211\n",
      "Episode 005 | Loss: 0.5189\n",
      "Episode 006 | Loss: -0.9433\n",
      "Episode 007 | Loss: -0.4621\n",
      "Episode 008 | Loss: 0.0372\n",
      "Episode 009 | Loss: -0.3840\n",
      "Episode 010 | Loss: 0.9301\n",
      "Episode 011 | Loss: 0.3736\n",
      "Episode 012 | Loss: 1.4039\n",
      "Episode 013 | Loss: 0.8700\n",
      "Episode 014 | Loss: -0.9375\n",
      "Episode 015 | Loss: -0.1025\n",
      "Episode 016 | Loss: -1.0211\n",
      "Episode 017 | Loss: -0.2897\n",
      "Episode 018 | Loss: 0.0635\n",
      "Episode 019 | Loss: 1.3083\n",
      "Episode 020 | Loss: 1.2413\n",
      "Episode 021 | Loss: -0.0265\n",
      "Episode 022 | Loss: 1.3342\n",
      "Episode 023 | Loss: -2.4558\n",
      "Episode 024 | Loss: 0.2154\n",
      "Episode 025 | Loss: -0.5026\n",
      "Episode 026 | Loss: 0.8787\n",
      "Episode 027 | Loss: -0.3820\n",
      "Episode 028 | Loss: -0.2196\n",
      "Episode 029 | Loss: -0.0451\n",
      "Episode 030 | Loss: -0.5375\n",
      "Episode 031 | Loss: 0.3166\n",
      "Episode 032 | Loss: -1.0678\n",
      "Episode 033 | Loss: -0.7930\n",
      "Episode 034 | Loss: 0.3560\n",
      "Episode 035 | Loss: -1.0768\n",
      "Episode 036 | Loss: -0.0890\n",
      "Episode 037 | Loss: -0.2783\n",
      "Episode 038 | Loss: -0.1830\n",
      "Episode 039 | Loss: 0.4951\n",
      "Episode 040 | Loss: -0.8129\n",
      "Episode 041 | Loss: -1.3436\n",
      "Episode 042 | Loss: 0.3892\n",
      "Episode 043 | Loss: -0.0865\n",
      "Episode 044 | Loss: 1.0599\n",
      "Episode 045 | Loss: 0.4746\n",
      "Episode 046 | Loss: -3.1330\n",
      "Episode 047 | Loss: -3.7182\n",
      "Episode 048 | Loss: 0.7452\n",
      "Episode 049 | Loss: -0.6813\n",
      "Episode 050 | Loss: 1.1749\n",
      "Episode 051 | Loss: -0.7490\n",
      "Episode 052 | Loss: -3.9807\n",
      "Episode 053 | Loss: -4.4464\n",
      "Episode 054 | Loss: -3.8651\n",
      "Episode 055 | Loss: 0.3283\n",
      "Episode 056 | Loss: 1.2936\n",
      "Episode 057 | Loss: 1.5638\n",
      "Episode 058 | Loss: -1.6437\n",
      "Episode 059 | Loss: -0.0893\n",
      "Episode 060 | Loss: 1.9116\n",
      "Episode 061 | Loss: -1.8520\n",
      "Episode 062 | Loss: 3.5082\n",
      "Episode 063 | Loss: -3.6568\n",
      "Episode 064 | Loss: 0.1980\n",
      "Episode 065 | Loss: -2.0372\n",
      "Episode 066 | Loss: -5.5816\n",
      "Episode 067 | Loss: -1.2397\n",
      "Episode 068 | Loss: -0.0937\n",
      "Episode 069 | Loss: 0.0123\n",
      "Episode 070 | Loss: -0.3451\n",
      "Episode 071 | Loss: 3.5845\n",
      "Episode 072 | Loss: 4.6535\n",
      "Episode 073 | Loss: 3.3577\n",
      "Episode 074 | Loss: -8.6600\n",
      "Episode 075 | Loss: -1.6678\n",
      "Episode 076 | Loss: -5.1945\n",
      "Episode 077 | Loss: 3.1821\n",
      "Episode 078 | Loss: 2.1524\n",
      "Episode 079 | Loss: 3.9973\n",
      "Episode 080 | Loss: -0.1114\n",
      "Episode 081 | Loss: 0.0024\n",
      "Episode 082 | Loss: 0.0179\n",
      "Episode 083 | Loss: -0.2345\n",
      "Episode 084 | Loss: -0.0297\n",
      "Episode 085 | Loss: -0.1483\n",
      "Episode 086 | Loss: -6.3084\n",
      "Episode 087 | Loss: 0.0470\n",
      "Episode 088 | Loss: -1.8069\n",
      "Episode 089 | Loss: 1.3817\n",
      "Episode 090 | Loss: 0.0012\n",
      "Episode 091 | Loss: -0.0743\n",
      "Episode 092 | Loss: -0.0531\n",
      "Episode 093 | Loss: -0.0023\n",
      "Episode 094 | Loss: 6.0904\n",
      "Episode 095 | Loss: -0.4182\n",
      "Episode 096 | Loss: 0.0206\n",
      "Episode 097 | Loss: -0.0060\n",
      "Episode 098 | Loss: -4.9497\n",
      "Episode 099 | Loss: 4.2121\n",
      "Episode 100 | Loss: 0.0533\n",
      "\n",
      "Evaluating the trained policy...\n",
      "Average Reward over 10 episodes: 1.46\n",
      "Policy deployed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "def main():\n",
    "    env = DummyEnv()\n",
    "    \n",
    "    # Generate random data and preprocess\n",
    "    data = np.random.rand(100, env.state_space)\n",
    "    processed_data = preprocess_data(data)\n",
    "\n",
    "    # Train-Test Split\n",
    "    train_data, test_data = train_test_split(processed_data)\n",
    "    print(f\"Data split: {len(train_data)} training samples, {len(test_data)} testing samples.\\n\")\n",
    "\n",
    "    # Initialize Policy and Optimizer\n",
    "    policy = PolicyNetwork(input_dim=env.state_space, output_dim=env.action_space)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "    print(\"Training policy using REINFORCE...\\n\")\n",
    "    train_reinforce(env, policy, optimizer, num_episodes=100)\n",
    "\n",
    "    # Step 5: Evaluate\n",
    "    print(\"\\nEvaluating the trained policy...\")\n",
    "    evaluate_policy(env, policy, num_episodes=10)\n",
    "\n",
    "    # Step 6: Deploy Policy\n",
    "    deploy_policy(policy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
