{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Reward: -1375.92\n",
      "Policy deployed and saved as 'sac_actor.pth'\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters ---\n",
    "ENV_NAME = 'Pendulum-v1'\n",
    "SEED = 42\n",
    "EPISODES = 5\n",
    "MAX_STEPS = 200\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "LR_ACTOR = 3e-4\n",
    "LR_CRITIC = 3e-4\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess_state(state):\n",
    "    return torch.FloatTensor(np.array(state)).unsqueeze(0)\n",
    "\n",
    "# --- Actor and Critic Networks ---\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU()\n",
    "        )\n",
    "        self.mean = nn.Linear(128, action_dim)\n",
    "        self.log_std = nn.Linear(128, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        mean = self.mean(x)\n",
    "        log_std = torch.clamp(self.log_std(x), -20, 2)\n",
    "        std = log_std.exp()\n",
    "        return mean, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        def critic_net():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(state_dim + action_dim, 128), nn.ReLU(),\n",
    "                nn.Linear(128, 128), nn.ReLU(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        self.q1 = critic_net()\n",
    "        self.q2 = critic_net()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=-1)\n",
    "        return self.q1(sa), self.q2(sa)\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.FloatTensor(np.array(actions)),\n",
    "            torch.FloatTensor(np.array(rewards)).unsqueeze(1),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(np.array(dones)).unsqueeze(1)\n",
    "        )\n",
    "\n",
    "# --- SAC Agent ---\n",
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.target_critic = Critic(state_dim, action_dim)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            mean, std = self.actor(state)\n",
    "            action = mean + std * torch.randn_like(std)\n",
    "            return action.clamp(-self.max_action, self.max_action).cpu().numpy().flatten()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer.buffer) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_mean, next_std = self.actor(next_states)\n",
    "            next_actions = next_mean + next_std * torch.randn_like(next_std)\n",
    "            q1_next, q2_next = self.target_critic(next_states, next_actions)\n",
    "            target_q = rewards + GAMMA * (1 - dones) * torch.min(q1_next, q2_next)\n",
    "\n",
    "        q1, q2 = self.critic(states, actions)\n",
    "        critic_loss = nn.MSELoss()(q1, target_q) + nn.MSELoss()(q2, target_q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        mean, std = self.actor(states)\n",
    "        new_actions = mean + std * torch.randn_like(std)\n",
    "        q1_new, _ = self.critic(states, new_actions)\n",
    "        actor_loss = -q1_new.mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n",
    "\n",
    "# --- Main Workflow ---\n",
    "env = gym.make(ENV_NAME)\n",
    "env.reset(seed=SEED)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "agent = SACAgent(state_dim, action_dim, max_action)\n",
    "train_episodes = int(EPISODES * TRAIN_TEST_SPLIT)\n",
    "test_episodes = EPISODES - train_episodes\n",
    "\n",
    "# Train Base Model\n",
    "for episode in range(train_episodes):\n",
    "    state, _ = env.reset()\n",
    "    for _ in range(MAX_STEPS):\n",
    "        state_tensor = preprocess_state(state)\n",
    "        action = agent.select_action(state_tensor)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        agent.train()\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# Fine-Tune Model\n",
    "LR_ACTOR /= 2\n",
    "LR_CRITIC /= 2\n",
    "agent.actor_optimizer = optim.Adam(agent.actor.parameters(), lr=LR_ACTOR)\n",
    "agent.critic_optimizer = optim.Adam(agent.critic.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "# Evaluate\n",
    "rewards = []\n",
    "for episode in range(test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    for _ in range(MAX_STEPS):\n",
    "        state_tensor = preprocess_state(state)\n",
    "        action = agent.select_action(state_tensor)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(f\"Average Test Reward: {np.mean(rewards):.2f}\")\n",
    "\n",
    "torch.save(agent.actor.state_dict(), 'sac_actor.pth')\n",
    "print(\"Policy deployed and saved as 'sac_actor.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
