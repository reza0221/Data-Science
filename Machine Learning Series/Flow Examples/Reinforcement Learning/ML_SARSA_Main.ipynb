{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "class RandomEnvironment:\n",
    "    def __init__(self, num_states=5, num_actions=2):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.transition_matrix = np.random.rand(num_states, num_actions, num_states)\n",
    "        self.transition_matrix /= self.transition_matrix.sum(axis=2, keepdims=True)  # Normalize\n",
    "        self.reward_matrix = np.random.rand(num_states, num_actions)  # Random rewards\n",
    "        self.state = np.random.randint(0, num_states)  # Start at a random state\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(self.num_states, p=self.transition_matrix[self.state, action])\n",
    "        reward = self.reward_matrix[self.state, action]\n",
    "        self.state = next_state\n",
    "        return next_state, reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(0, self.num_states)\n",
    "        return self.state\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_environment(env):\n",
    "    return env.num_states, env.num_actions\n",
    "\n",
    "# Train-Test Split\n",
    "def train_test_split(data, split_ratio=0.8):\n",
    "    split_idx = int(len(data) * split_ratio)\n",
    "    return data[:split_idx], data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA Algorithm Class\n",
    "class SARSA:\n",
    "    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.q_table = np.zeros((num_states, num_actions))  # Initialize Q-table\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)  # Explore\n",
    "        return np.argmax(self.q_table[state])  # Exploit\n",
    "    \n",
    "    def train(self, env):\n",
    "        for _ in range(self.episodes):\n",
    "            state = env.reset()\n",
    "            action = self.choose_action(state)\n",
    "            while True:\n",
    "                next_state, reward = env.step(action)\n",
    "                next_action = self.choose_action(next_state)\n",
    "                # SARSA update rule\n",
    "                self.q_table[state, action] += self.alpha * (\n",
    "                    reward + self.gamma * self.q_table[next_state, next_action] - self.q_table[state, action]\n",
    "                )\n",
    "                if random.uniform(0, 1) < 0.1:  # Random stopping condition for demo\n",
    "                    break\n",
    "                state, action = next_state, next_action\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return np.mean(self.q_table)  # Simple evaluation metric\n",
    "    \n",
    "    def deploy_policy(self):\n",
    "        return np.argmax(self.q_table, axis=1)  # Best actions for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Q-Value: 6.6275134896880985\n",
      "Optimal Policy: [0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    env = RandomEnvironment()\n",
    "    num_states, num_actions = preprocess_environment(env)\n",
    "    \n",
    "    # Train SARSA Model\n",
    "    sarsa_agent = SARSA(num_states, num_actions)\n",
    "    sarsa_agent.train(env)\n",
    "    \n",
    "    # Evaluate Model\n",
    "    avg_q_value = sarsa_agent.evaluate()\n",
    "    print(f\"Average Q-Value: {avg_q_value}\")\n",
    "    \n",
    "    # Deploy Policy\n",
    "    optimal_policy = sarsa_agent.deploy_policy()\n",
    "    print(f\"Optimal Policy: {optimal_policy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
