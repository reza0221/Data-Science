{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/160: Reward = -1146.41\n",
      "Episode 2/160: Reward = -1072.43\n",
      "Episode 3/160: Reward = -1398.38\n",
      "Episode 4/160: Reward = -1858.73\n",
      "Episode 5/160: Reward = -1566.06\n",
      "Episode 6/160: Reward = -1588.78\n",
      "Episode 7/160: Reward = -1498.01\n",
      "Episode 8/160: Reward = -1484.39\n",
      "Episode 9/160: Reward = -1565.92\n",
      "Episode 10/160: Reward = -1306.10\n",
      "Episode 11/160: Reward = -1478.86\n",
      "Episode 12/160: Reward = -1402.98\n",
      "Episode 13/160: Reward = -1501.75\n",
      "Episode 14/160: Reward = -1334.18\n",
      "Episode 15/160: Reward = -1463.20\n",
      "Episode 16/160: Reward = -1580.51\n",
      "Episode 17/160: Reward = -1551.30\n",
      "Episode 18/160: Reward = -1538.05\n",
      "Episode 19/160: Reward = -1574.26\n",
      "Episode 20/160: Reward = -1139.00\n",
      "Episode 21/160: Reward = -1154.60\n",
      "Episode 22/160: Reward = -1163.12\n",
      "Episode 23/160: Reward = -1294.75\n",
      "Episode 24/160: Reward = -1211.27\n",
      "Episode 25/160: Reward = -1113.14\n",
      "Episode 26/160: Reward = -1002.39\n",
      "Episode 27/160: Reward = -1020.69\n",
      "Episode 28/160: Reward = -900.82\n",
      "Episode 29/160: Reward = -934.16\n",
      "Episode 30/160: Reward = -1002.15\n",
      "Episode 31/160: Reward = -1028.27\n",
      "Episode 32/160: Reward = -1069.29\n",
      "Episode 33/160: Reward = -1067.13\n",
      "Episode 34/160: Reward = -989.75\n",
      "Episode 35/160: Reward = -811.81\n",
      "Episode 36/160: Reward = -988.74\n",
      "Episode 37/160: Reward = -1070.43\n",
      "Episode 38/160: Reward = -928.78\n",
      "Episode 39/160: Reward = -635.54\n",
      "Episode 40/160: Reward = -701.58\n",
      "Episode 41/160: Reward = -633.32\n",
      "Episode 42/160: Reward = -1112.20\n",
      "Episode 43/160: Reward = -905.56\n",
      "Episode 44/160: Reward = -972.70\n",
      "Episode 45/160: Reward = -917.13\n",
      "Episode 46/160: Reward = -880.26\n",
      "Episode 47/160: Reward = -914.51\n",
      "Episode 48/160: Reward = -888.68\n",
      "Episode 49/160: Reward = -904.50\n",
      "Episode 50/160: Reward = -1211.23\n",
      "Episode 51/160: Reward = -1074.12\n",
      "Episode 52/160: Reward = -1208.03\n",
      "Episode 53/160: Reward = -991.22\n",
      "Episode 54/160: Reward = -1051.66\n",
      "Episode 55/160: Reward = -1114.88\n",
      "Episode 56/160: Reward = -1175.07\n",
      "Episode 57/160: Reward = -1161.42\n",
      "Episode 58/160: Reward = -1354.41\n",
      "Episode 59/160: Reward = -1073.83\n",
      "Episode 60/160: Reward = -1085.01\n",
      "Episode 61/160: Reward = -1241.85\n",
      "Episode 62/160: Reward = -920.01\n",
      "Episode 63/160: Reward = -895.70\n",
      "Episode 64/160: Reward = -1200.58\n",
      "Episode 65/160: Reward = -791.71\n",
      "Episode 66/160: Reward = -1057.16\n",
      "Episode 67/160: Reward = -909.78\n",
      "Episode 68/160: Reward = -1191.17\n",
      "Episode 69/160: Reward = -861.47\n",
      "Episode 70/160: Reward = -1130.39\n",
      "Episode 71/160: Reward = -1213.83\n",
      "Episode 72/160: Reward = -752.29\n",
      "Episode 73/160: Reward = -892.69\n",
      "Episode 74/160: Reward = -649.84\n",
      "Episode 75/160: Reward = -1189.40\n",
      "Episode 76/160: Reward = -1248.81\n",
      "Episode 77/160: Reward = -1352.50\n",
      "Episode 78/160: Reward = -1202.80\n",
      "Episode 79/160: Reward = -689.38\n",
      "Episode 80/160: Reward = -1208.92\n",
      "Episode 81/160: Reward = -503.02\n",
      "Episode 82/160: Reward = -913.39\n",
      "Episode 83/160: Reward = -892.37\n",
      "Episode 84/160: Reward = -1019.38\n",
      "Episode 85/160: Reward = -939.74\n",
      "Episode 86/160: Reward = -1046.48\n",
      "Episode 87/160: Reward = -889.59\n",
      "Episode 88/160: Reward = -1048.41\n",
      "Episode 89/160: Reward = -768.15\n",
      "Episode 90/160: Reward = -905.78\n",
      "Episode 91/160: Reward = -778.72\n",
      "Episode 92/160: Reward = -1515.64\n",
      "Episode 93/160: Reward = -253.49\n",
      "Episode 94/160: Reward = -126.52\n",
      "Episode 95/160: Reward = -1.67\n",
      "Episode 96/160: Reward = -249.91\n",
      "Episode 97/160: Reward = -253.81\n",
      "Episode 98/160: Reward = -118.40\n",
      "Episode 99/160: Reward = -0.35\n",
      "Episode 100/160: Reward = -124.80\n",
      "Episode 101/160: Reward = -122.01\n",
      "Episode 102/160: Reward = -245.59\n",
      "Episode 103/160: Reward = -122.94\n",
      "Episode 104/160: Reward = -126.46\n",
      "Episode 105/160: Reward = -250.57\n",
      "Episode 106/160: Reward = -353.01\n",
      "Episode 107/160: Reward = -355.30\n",
      "Episode 108/160: Reward = -0.14\n",
      "Episode 109/160: Reward = -356.93\n",
      "Episode 110/160: Reward = -2.65\n",
      "Episode 111/160: Reward = -0.50\n",
      "Episode 112/160: Reward = -592.73\n",
      "Episode 113/160: Reward = -360.50\n",
      "Episode 114/160: Reward = -126.32\n",
      "Episode 115/160: Reward = -242.03\n",
      "Episode 116/160: Reward = -5.17\n",
      "Episode 117/160: Reward = -122.79\n",
      "Episode 118/160: Reward = -0.20\n",
      "Episode 119/160: Reward = -124.04\n",
      "Episode 120/160: Reward = -249.62\n",
      "Episode 121/160: Reward = -123.27\n",
      "Episode 122/160: Reward = -123.97\n",
      "Episode 123/160: Reward = -458.71\n",
      "Episode 124/160: Reward = -122.34\n",
      "Episode 125/160: Reward = -123.82\n",
      "Episode 126/160: Reward = -0.95\n",
      "Episode 127/160: Reward = -124.03\n",
      "Episode 128/160: Reward = -124.49\n",
      "Episode 129/160: Reward = -127.10\n",
      "Episode 130/160: Reward = -124.42\n",
      "Episode 131/160: Reward = -232.11\n",
      "Episode 132/160: Reward = -1.01\n",
      "Episode 133/160: Reward = -359.04\n",
      "Episode 134/160: Reward = -130.36\n",
      "Episode 135/160: Reward = -236.83\n",
      "Episode 136/160: Reward = -247.07\n",
      "Episode 137/160: Reward = -2.79\n",
      "Episode 138/160: Reward = -124.17\n",
      "Episode 139/160: Reward = -127.75\n",
      "Episode 140/160: Reward = -130.60\n",
      "Episode 141/160: Reward = -351.44\n",
      "Episode 142/160: Reward = -1.54\n",
      "Episode 143/160: Reward = -126.85\n",
      "Episode 144/160: Reward = -124.54\n",
      "Episode 145/160: Reward = -125.45\n",
      "Episode 146/160: Reward = -127.12\n",
      "Episode 147/160: Reward = -124.59\n",
      "Episode 148/160: Reward = -325.12\n",
      "Episode 149/160: Reward = -239.52\n",
      "Episode 150/160: Reward = -126.30\n",
      "Episode 151/160: Reward = -235.91\n",
      "Episode 152/160: Reward = -119.36\n",
      "Episode 153/160: Reward = -362.27\n",
      "Episode 154/160: Reward = -128.12\n",
      "Episode 155/160: Reward = -244.55\n",
      "Episode 156/160: Reward = -374.88\n",
      "Episode 157/160: Reward = -120.07\n",
      "Episode 158/160: Reward = -240.09\n",
      "Episode 159/160: Reward = -128.85\n",
      "Episode 160/160: Reward = -114.42\n",
      "Fine-tuning model...\n",
      "Episode 1/20: Reward = -235.10\n",
      "Episode 2/20: Reward = -245.94\n",
      "Episode 3/20: Reward = -2.32\n",
      "Episode 4/20: Reward = -125.29\n",
      "Episode 5/20: Reward = -364.41\n",
      "Episode 6/20: Reward = -122.72\n",
      "Episode 7/20: Reward = -360.14\n",
      "Episode 8/20: Reward = -320.40\n",
      "Episode 9/20: Reward = -124.31\n",
      "Episode 10/20: Reward = -244.05\n",
      "Episode 11/20: Reward = -119.53\n",
      "Episode 12/20: Reward = -224.58\n",
      "Episode 13/20: Reward = -123.46\n",
      "Episode 14/20: Reward = -122.35\n",
      "Episode 15/20: Reward = -265.56\n",
      "Episode 16/20: Reward = -117.68\n",
      "Episode 17/20: Reward = -254.96\n",
      "Episode 18/20: Reward = -1518.74\n",
      "Episode 19/20: Reward = -372.12\n",
      "Episode 20/20: Reward = -126.36\n",
      "Test Episode 1/20: Reward = -123.97\n",
      "Test Episode 2/20: Reward = -120.41\n",
      "Test Episode 3/20: Reward = -123.21\n",
      "Test Episode 4/20: Reward = -0.70\n",
      "Test Episode 5/20: Reward = -118.69\n",
      "Test Episode 6/20: Reward = -118.56\n",
      "Test Episode 7/20: Reward = -122.53\n",
      "Test Episode 8/20: Reward = -123.28\n",
      "Test Episode 9/20: Reward = -120.85\n",
      "Test Episode 10/20: Reward = -122.41\n",
      "Test Episode 11/20: Reward = -350.06\n",
      "Test Episode 12/20: Reward = -275.58\n",
      "Test Episode 13/20: Reward = -228.86\n",
      "Test Episode 14/20: Reward = -124.05\n",
      "Test Episode 15/20: Reward = -123.81\n",
      "Test Episode 16/20: Reward = -234.99\n",
      "Test Episode 17/20: Reward = -339.98\n",
      "Test Episode 18/20: Reward = -1.86\n",
      "Test Episode 19/20: Reward = -114.23\n",
      "Test Episode 20/20: Reward = -235.50\n",
      "Average Test Reward: -156.18\n",
      "Deploying policy...\n"
     ]
    }
   ],
   "source": [
    "# ---- Environment ----\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")  # Lightweight continuous control environment\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = float(env.action_space.high[0])\n",
    "\n",
    "# ---- Preprocessing ----\n",
    "def preprocess_state(state):\n",
    "    return torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "# ---- Actor and Critic Networks ----\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.fc(state) * self.action_bound\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.fc(torch.cat([state, action], dim=-1))\n",
    "\n",
    "# ---- Replay Buffer ----\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "        return (\n",
    "            torch.tensor(np.array(states), dtype=torch.float32),\n",
    "            torch.tensor(np.array(actions), dtype=torch.float32),\n",
    "            torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1),\n",
    "            torch.tensor(np.array(next_states), dtype=torch.float32),\n",
    "            torch.tensor(np.array(dones), dtype=torch.float32).unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "# ---- Train-Test Split ----\n",
    "train_ratio = 0.8\n",
    "train_episodes = int(200 * train_ratio)\n",
    "val_episodes = int(200 * 0.1)\n",
    "test_episodes = 200 - train_episodes - val_episodes\n",
    "\n",
    "# ---- TD3 Hyperparameters ----\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-3\n",
    "discount = 0.99\n",
    "tau = 0.005\n",
    "policy_noise = 0.2\n",
    "noise_clip = 0.5\n",
    "policy_freq = 2\n",
    "batch_size = 64\n",
    "\n",
    "# ---- Initialize Networks and Optimizers ----\n",
    "actor = Actor(state_dim, action_dim, action_bound)\n",
    "actor_target = Actor(state_dim, action_dim, action_bound)\n",
    "critic1 = Critic(state_dim, action_dim)\n",
    "critic1_target = Critic(state_dim, action_dim)\n",
    "critic2 = Critic(state_dim, action_dim)\n",
    "critic2_target = Critic(state_dim, action_dim)\n",
    "\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic1_target.load_state_dict(critic1.state_dict())\n",
    "critic2_target.load_state_dict(critic2.state_dict())\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "critic1_optimizer = optim.Adam(critic1.parameters(), lr=critic_lr)\n",
    "critic2_optimizer = optim.Adam(critic2.parameters(), lr=critic_lr)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# ---- Utilities ----\n",
    "def soft_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "# ---- Train Base Model (TD3) ----\n",
    "def train_td3(episodes):\n",
    "    for episode in range(episodes):\n",
    "        state = preprocess_state(env.reset()[0])\n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = actor(state).detach().cpu().numpy()\n",
    "            noise = np.random.normal(0, policy_noise, size=action_dim)\n",
    "            action = np.clip(action + noise, -action_bound, action_bound)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state.numpy(), action, reward, next_state, done)\n",
    "            state = preprocess_state(next_state)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if len(replay_buffer.buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    noise = (torch.randn_like(actions) * policy_noise).clamp(-noise_clip, noise_clip)\n",
    "                    next_actions = (actor_target(next_states) + noise).clamp(-action_bound, action_bound)\n",
    "                    target_q1 = critic1_target(next_states, next_actions)\n",
    "                    target_q2 = critic2_target(next_states, next_actions)\n",
    "                    target_q = rewards + discount * (1 - dones) * torch.min(target_q1, target_q2)\n",
    "\n",
    "                current_q1 = critic1(states, actions)\n",
    "                current_q2 = critic2(states, actions)\n",
    "\n",
    "                critic1_loss = nn.MSELoss()(current_q1, target_q)\n",
    "                critic2_loss = nn.MSELoss()(current_q2, target_q)\n",
    "\n",
    "                critic1_optimizer.zero_grad()\n",
    "                critic1_loss.backward()\n",
    "                critic1_optimizer.step()\n",
    "\n",
    "                critic2_optimizer.zero_grad()\n",
    "                critic2_loss.backward()\n",
    "                critic2_optimizer.step()\n",
    "\n",
    "                if episode % policy_freq == 0:\n",
    "                    actor_loss = -critic1(states, actor(states)).mean()\n",
    "                    actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_optimizer.step()\n",
    "\n",
    "                    soft_update(actor_target, actor)\n",
    "                    soft_update(critic1_target, critic1)\n",
    "                    soft_update(critic2_target, critic2)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{episodes}: Reward = {episode_reward:.2f}\")\n",
    "\n",
    "train_td3(train_episodes)\n",
    "\n",
    "# ---- Fine-Tune Model ----\n",
    "def fine_tune_model(episodes):\n",
    "    print(\"Fine-tuning model...\")\n",
    "    train_td3(episodes)\n",
    "\n",
    "fine_tune_model(val_episodes)\n",
    "\n",
    "# ---- Evaluate ----\n",
    "def evaluate_model(episodes):\n",
    "    total_reward = 0.0\n",
    "    for episode in range(episodes):\n",
    "        state = preprocess_state(env.reset()[0])\n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = actor(state).detach().cpu().numpy()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = preprocess_state(next_state)\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Test Episode {episode + 1}/{episodes}: Reward = {episode_reward:.2f}\")\n",
    "\n",
    "    avg_reward = total_reward / episodes\n",
    "    print(f\"Average Test Reward: {avg_reward:.2f}\")\n",
    "\n",
    "evaluate_model(test_episodes)\n",
    "\n",
    "# ---- Deploy Policy ----\n",
    "def deploy_policy():\n",
    "    print(\"Deploying policy...\")\n",
    "    for episode in range(3):\n",
    "        state = preprocess_state(env.reset()[0])\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = actor(state).detach().cpu().numpy()\n",
    "            next_state, _, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = preprocess_state(next_state)\n",
    "    env.close()\n",
    "\n",
    "deploy_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
