{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Random Transaction Data\n",
    "np.random.seed(42)\n",
    "n_transactions = 100\n",
    "n_items = 10\n",
    "\n",
    "# Create a random binary transaction dataset (1 = item bought, 0 = not bought)\n",
    "data = np.random.choice([0, 1], size=(n_transactions, n_items), p=[0.7, 0.3])\n",
    "columns = [f\"Item_{i+1}\" for i in range(n_items)]\n",
    "df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocessing\n",
    "# Convert transaction data into a list of itemsets\n",
    "transactions = []\n",
    "for i in range(len(df)):\n",
    "    transactions.append(set(df.columns[df.iloc[i] == 1]))\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "train_transactions, test_transactions = train_test_split(transactions, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Eclat Algorithm (Recursive Depth-First Search)\n",
    "def get_support(itemset, transactions):\n",
    "    \"\"\" Calculate support of an itemset \"\"\"\n",
    "    return sum(1 for transaction in transactions if itemset.issubset(transaction)) / len(transactions)\n",
    "\n",
    "def eclat(prefix, items, transactions, min_support, freq_itemsets):\n",
    "    \"\"\" Recursive Eclat Algorithm \"\"\"\n",
    "    while items:\n",
    "        item = items.pop()\n",
    "        new_itemset = prefix | {item}\n",
    "        support = get_support(new_itemset, transactions)\n",
    "        \n",
    "        if support >= min_support:\n",
    "            freq_itemsets[frozenset(new_itemset)] = support\n",
    "            remaining_items = {i for i in items if i > item}\n",
    "            eclat(new_itemset, remaining_items, transactions, min_support, freq_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find frequent itemsets using Eclat\n",
    "min_support = 0.1  # Adjusted to ensure more rules\n",
    "freq_itemsets = {}\n",
    "unique_items = {item for transaction in train_transactions for item in transaction}\n",
    "eclat(set(), unique_items, train_transactions, min_support, freq_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert frequent itemsets to DataFrame\n",
    "frequent_itemsets_df = pd.DataFrame(\n",
    "    [(set(itemset), support) for itemset, support in freq_itemsets.items()],\n",
    "    columns=['itemset', 'support']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate Association Rules\n",
    "def generate_association_rules(freq_itemsets, min_confidence=0.4):\n",
    "    \"\"\" Generate association rules from frequent itemsets \"\"\"\n",
    "    rules = []\n",
    "    for itemset, support in freq_itemsets.items():\n",
    "        if len(itemset) > 1:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedent in combinations(itemset, i):\n",
    "                    antecedent = set(antecedent)\n",
    "                    consequent = itemset - antecedent\n",
    "                    confidence = support / freq_itemsets[frozenset(antecedent)]\n",
    "                    \n",
    "                    if confidence >= min_confidence:\n",
    "                        lift = confidence / get_support(consequent, train_transactions)\n",
    "                        rules.append((antecedent, consequent, support, confidence, lift))\n",
    "    \n",
    "    return pd.DataFrame(rules, columns=['antecedents', 'consequents', 'support', 'confidence', 'lift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Association Rules:\n",
      "  antecedents consequents  support  confidence      lift\n",
      "0    {Item_9}    (Item_6)   0.1625    0.448276  1.379310\n",
      "1    {Item_6}    (Item_9)   0.1625    0.500000  1.379310\n",
      "2   {Item_10}    (Item_9)   0.1375    0.458333  1.264368\n",
      "3    {Item_5}   (Item_10)   0.1000    0.400000  1.333333\n",
      "4    {Item_3}    (Item_2)   0.1250    0.526316  1.503759\n"
     ]
    }
   ],
   "source": [
    "# Generate rules\n",
    "rules_df = generate_association_rules(freq_itemsets, min_confidence=0.4)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "print(\"\\nGenerated Association Rules:\")\n",
    "if not rules_df.empty:\n",
    "    print(rules_df)\n",
    "else:\n",
    "    print(\"No association rules generated. Try lowering min_support or min_confidence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Associations for New Transaction:\n",
      "  antecedents consequents  confidence      lift\n",
      "3    {Item_5}   (Item_10)    0.400000  1.333333\n",
      "4    {Item_3}    (Item_2)    0.526316  1.503759\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Predict New Data\n",
    "new_transaction = np.random.choice([0, 1], size=(1, n_items), p=[0.7, 0.3])\n",
    "new_transaction_df = pd.DataFrame(new_transaction, columns=columns).astype(bool)\n",
    "new_items = set(new_transaction_df.columns[new_transaction[0] == 1])\n",
    "\n",
    "if not rules_df.empty:\n",
    "    matching_rules = rules_df[rules_df['antecedents'].apply(lambda x: x.issubset(new_items))]\n",
    "    \n",
    "    print(\"\\nPredicted Associations for New Transaction:\")\n",
    "    if not matching_rules.empty:\n",
    "        print(matching_rules[['antecedents', 'consequents', 'confidence', 'lift']])\n",
    "    else:\n",
    "        print(\"No association rules available for prediction.\")\n",
    "else:\n",
    "    print(\"No association rules generated to make predictions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
