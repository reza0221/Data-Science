{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Simulation (Random Sentences)\n",
    "random_sentences = [\n",
    "    \"The cat sat on the mat\", \"The dog barked loudly\", \n",
    "    \"She loves programming\", \"The sky is blue\", \n",
    "    \"He enjoys reading books\", \"Birds are flying high\", \n",
    "    \"The sun is shining bright\", \"I like watching movies\",\n",
    "    \"They are playing football\", \"He is eating dinner\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocessing\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def mask_tokens(sentences, tokenizer):\n",
    "    \"\"\"Tokenizes sentences and randomly masks a word in each sentence.\"\"\"\n",
    "    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "    labels = inputs.input_ids.clone()  # Keep a copy of original inputs for labels\n",
    "\n",
    "    # Randomly select one token per sentence to be masked (except special tokens)\n",
    "    mask_indices = torch.randint(1, inputs.input_ids.shape[1] - 1, (inputs.input_ids.shape[0],))\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        labels[i, :] = -100  # Default ignore index for loss calculation\n",
    "        labels[i, mask_indices[i]] = inputs.input_ids[i, mask_indices[i]]  # Only mask one token\n",
    "        inputs.input_ids[i, mask_indices[i]] = tokenizer.mask_token_id  # Replace with [MASK]\n",
    "\n",
    "    return inputs.input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masked input and labels\n",
    "masked_inputs, labels = mask_tokens(random_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(masked_inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.3489\n",
      "Epoch 2, Loss: 6.2921\n",
      "Epoch 3, Loss: 5.1332\n",
      "Epoch 4, Loss: 4.3844\n",
      "Epoch 5, Loss: 3.6391\n",
      "Epoch 6, Loss: 2.8742\n",
      "Epoch 7, Loss: 2.7872\n",
      "Epoch 8, Loss: 2.6044\n",
      "Epoch 9, Loss: 2.3988\n",
      "Epoch 10, Loss: 2.1033\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train Base Model (Masked Language Modeling)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "epochs = 10  # Increase training time\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=inputs, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the Model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test.to(device))\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1).cpu()\n",
    "\n",
    "    # Compute accuracy only on masked tokens\n",
    "    mask_indices = (y_test != -100)  # Get positions where actual words were masked\n",
    "    correct = (predictions[mask_indices] == y_test[mask_indices]).sum().item()\n",
    "    total = mask_indices.sum().item()\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentences: ['. the weather is nice day.', '. it is learning machine..']\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Predict New Data\n",
    "new_sentences = [\"The weather is nice today\", \"She is learning machine learning\"]\n",
    "new_inputs, _ = mask_tokens(new_sentences, tokenizer)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_outputs = model(new_inputs.to(device))\n",
    "    new_predictions = torch.argmax(new_outputs.logits, dim=-1)\n",
    "    predicted_tokens = [tokenizer.decode(new_predictions[i], skip_special_tokens=True) for i in range(len(new_sentences))]\n",
    "\n",
    "print(f\"Predicted sentences: {predicted_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
