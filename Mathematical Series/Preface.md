# Mathematics for Data Science
## Linear Algebra
### Basics
- Scalars
  - Basic Operations
  - Operations Related to Vector
  - Properties & Combinations
  - Statistical Context
- Vectors
  - Basic Vector Arithmetic
  - Vector Products
  - Vector Properties & Transformations
  - Vector-Based Geometric Interpretations
  - Vector Operations in Higher Dimensions
- Matrices
  - Matrix Addition
  - Matrix Subtraction
  - Scalar Multiplication
  - Matrix Multiplication (Dot Product)
  - Element-wise Multiplication
  - Matrix Transposition
  - Matrix Determinant Calculation
  - Matrix Inversion
  - Matrix Rank Calculation
  - Eigenvalues and Eigenvectors Calculation
- Tensors
  - Tensor Addition
  - Tensor Subtraction
  - Scalar Multiplication
  - Tensor Multiplication (Dot Product & Matrix Product)
  - Element-wise Multiplication
  - Tensor Transposition (Permutation of Axes)
  - Tensor Contraction (Reduction via Summation over Axes)
- Vector Spaces
  - Real Vector Space
  - Complex Vector Space
  - Euclidean Space (ℝⁿ Space)
  - Function Space
  - Polynomial Vector Space
  - Matrix Vector Space
  - Normed Vector Space
- Linear Combinations and Span
  - Basic Linear Combination
  - Checking if a Vector is in the Span
  - Span of Two Vectors
  - Span of a Set of Vectors
  - Basis and Dimension of a Span
- Linear Transformations
  - Scaling Transformation
  - Rotation Transformation
  - Reflection Transformation
  - Shear Transformation
  - Projection Transformation
  - Identity Transformation
  
### Advanced
- Dot Product and Cross Product
  - Dot Product
    - Two-Dimensional Vectors
    - Three-Dimensional Vectors
    - Dot Product Application in Projection
    - Angle Between Two Vectors Using Dot Product
    - Work Done by a Force
  - Cross Product
    - Three-Dimensional Vectors
    - Geometric Interpretation of Cross Product (Area of Parallelogram)
    - Torque Calculation Using Cross Product
    - Perpendicular Vectors and Cross Product
    - Unit Vector Normal to Two Vectors Using Cross Product
- Norms and Distances
  - L1 Norm (Manhattan Distance or Taxicab Norm)
  - L2 Norm (Euclidean Distance)
  - Lp Norm (Generalized Form of Norms)
  - Max Norm (Infinity Norm, Chebyshev Distance)
  - Frobenius Norm (Matrix Norm)
  - Cosine Similarity (Angular Distance in High Dimensions)
  - Mahalanobis Distance (Statistical Distance)
- Eigenvalues and Eigenvectors
  - Basic Eigenvalue and Eigenvector Calculation
  - Diagonalization Using Eigenvalues and Eigenvectors
  - Symmetric Matrix Eigenvalues
  - Singular Matrix Eigenvalue
  - Eigenvalues in Markov Chains
  - Eigenvalues and Eigenvectors in PCA
- Matrix Decomposition
  - LU Decomposition
    - General square matrix LU decomposition
    - LU decomposition with partial pivoting
  - QR Decomposition
    - QR decomposition for a full-rank square matrix
    - QR decomposition for a rectangular matrix
  - SV Decomposition
    - Full SVD for a square matrix
    - SVD for a rectangular matrix
    
## Calculus
### Basic Concepts
- Limits and Continuity
- Differentiation
- Integration
- Fundamental Theorem of Calculus
  
### Multivariable Calculus
- Partial Derivatives
- Gradient, Divergent, and Curl
- Multiple Integrals
- Jacobian and Change of Variables
  
### Advanved Concepts
- Vector Calculus
  - Line Integrals
  - Surface Integrals
- Taylor Series and Approximation
- Optimization Techniques
  - Lagrange Multipliers
  - Constrained Optimization
- Differential Equations
  - Ordinary Differential Equations
  - Partial Differenrial Equations
  - Calculus of Variations

## Optimization
### Basic Concepts
- Objective Functions
- Convex and Non-Convex Functions
- Global vs. Local Minima/Maxima
- Gradient Descent (Basic)

### Constrained Optimization
- Linear Programming (LP)
- Quadratic Programming (QP)
- Lagrange Multipliers (Detailed)
- KKT Conditions (Karush-Kuhn-Tucker)

### Gradient-Based Variants
- Gradient Descents
  - SGD (Stochastic Gradient Descent)
  - Mini-Batch Gradient Descent
  - RMSProp (Root Mean Square Propagation)
- Momentum-Based Methods
  - Momentum
  - Nesterov Accelerated Gradient (NAG)
- Hybrid Variant (Gradient Descents & Momentum-based Methods)
  - Adam (Adaptive Moment Estimation)
- Newton’s Method and Quasi-Newton Methods

### Optimization for Machine Learning
- Stochastic Optimization
- Regularization Techniques
  - L2 (Ridge Regularization)
  - L1 (Lasso Regularization)
  - Elastic Net
  - Dropout
  - Early Stopping
- Hyperparameter Optimization
  - Grid Search
  - Random Search
  - Bayesian Optimization
  - Evolutionary Algorithms
    
### Advanced Optimization Topics
- Conjugate Gradient Method
- Trust-Region Methods
- Simulated Annealing
- Genetic Algorithms
- Coordinate Descent
  
### Optimization in Deep Learning
- Backpropagation and Gradient Flow
- Batch Normalization and Learning Rate Schedules
- Auto-Differentiation
