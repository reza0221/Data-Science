# Mathematics for Data Science
## Linear Algebra
### Basics
- Scalars
  - Basic Operations
  - Operations Related to Vector
  - Properties & Combinations
  - Statistical Context
- Vectors
  - Basic Vector Arithmetic
  - Vector Products
  - Vector Properties & Transformations
  - Vector-Based Geometric Interpretations
  - Vector Operations in Higher Dimensions
- Matrices
  - Matrix Addition
  - Matrix Subtraction
  - Scalar Multiplication
  - Matrix Multiplication (Dot Product)
  - Element-wise Multiplication
  - Matrix Transposition
  - Matrix Determinant Calculation
  - Matrix Inversion
  - Matrix Rank Calculation
  - Eigenvalues and Eigenvectors Calculation
- Tensors
  - Tensor Addition
  - Tensor Subtraction
  - Scalar Multiplication
  - Tensor Multiplication (Dot Product & Matrix Product)
  - Element-wise Multiplication
  - Tensor Transposition (Permutation of Axes)
  - Tensor Contraction (Reduction via Summation over Axes)
- Vector Spaces
  - Real Vector Space
  - Complex Vector Space
  - Euclidean Space (ℝⁿ Space)
  - Function Space
  - Polynomial Vector Space
  - Matrix Vector Space
  - Normed Vector Space
- Linear Combinations and Span
  - Basic Linear Combination
  - Checking if a Vector is in the Span
  - Span of Two Vectors
  - Span of a Set of Vectors
  - Basis and Dimension of a Span
- Linear Transformations
  - Scaling Transformation
  - Rotation Transformation
  - Reflection Transformation
  - Shear Transformation
  - Projection Transformation
  - Identity Transformation
  
### Advanced
- Dot Product and Cross Product
  - Dot Product
    - Two-Dimensional Vectors
    - Three-Dimensional Vectors
    - Dot Product Application in Projection
    - Angle Between Two Vectors Using Dot Product
    - Work Done by a Force
  - Cross Product
    - Three-Dimensional Vectors
    - Geometric Interpretation of Cross Product (Area of Parallelogram)
    - Torque Calculation Using Cross Product
    - Perpendicular Vectors and Cross Product
    - Unit Vector Normal to Two Vectors Using Cross Product
- Norms and Distances
  - L1 Norm (Manhattan Distance or Taxicab Norm)
  - L2 Norm (Euclidean Distance)
  - Lp Norm (Generalized Form of Norms)
  - Max Norm (Infinity Norm, Chebyshev Distance)
  - Frobenius Norm (Matrix Norm)
  - Cosine Similarity (Angular Distance in High Dimensions)
  - Mahalanobis Distance (Statistical Distance)
- Eigenvalues and Eigenvectors
  - Basic Eigenvalue and Eigenvector Calculation
  - Diagonalization Using Eigenvalues and Eigenvectors
  - Symmetric Matrix Eigenvalues
  - Singular Matrix Eigenvalue
  - Eigenvalues in Markov Chains
  - Eigenvalues and Eigenvectors in PCA
- Matrix Decomposition
  - LU Decomposition
    - General square matrix LU decomposition
    - LU decomposition with partial pivoting
  - QR Decomposition
    - QR decomposition for a full-rank square matrix
    - QR decomposition for a rectangular matrix
  - SV Decomposition
    - Full SVD for a square matrix
    - SVD for a rectangular matrix
    
## Calculus
### Basic Concepts
- Limits and Continuity
  - Basic Limit Calculation
  - One-Sided Limits
  - Limit at Infinity
  - Indeterminate Forms and L'Hôpital's Rule
  - Squeeze Theorem
  - Continuity at a Point
  - Discontinuities
  - Piecewise Function Continuity
- Differentiation
  - Basic Derivatives (Power Rule, Sum Rule, Constant Rule)
  - Product Rule
  - Quotient Rule
  - Chain Rule
  - Higher-Order Derivatives
  - Implicit Differentiation
  - Logarithmic Differentiation
  - Partial Derivatives (for multivariable functions)
  - Directional Derivatives
  - Gradient and Hessian Matrix
- Integration
  - Indefinite Integrals (Antiderivatives)
  - Definite Integrals (With Limits)
  - Integration by Substitution (u-Substitution)
  - Integration by Parts
  - Trigonometric Integrals (Using Trigonometric Identities)
  - Improper Integrals (Infinite Limits or Discontinuous Functions)
  - Double Integrals (Multivariable Integration)
  - Triple Integrals (Extending to 3D Volume Computations)
- Fundamental Theorem of Calculus (FTC)
  - First Fundamental Theorem of Calculus (FTC-1)
  - Second Fundamental Theorem of Calculus (FTC-2)
  
### Multivariable Calculus
- Partial Derivatives
  - First-Order Partial Derivative (∂f/∂x and ∂f/∂y)
  - Second-Order Partial Derivative (∂²f/∂x², ∂²f/∂y², and mixed derivatives ∂²f/∂x∂y)
  - Gradient (∇f) (Vector of first-order partial derivatives)
  - Directional Derivative (Rate of change in a given direction)
  - Jacobian Matrix (Matrix of first-order partial derivatives for multivariable functions)
  - Hessian Matrix (Square matrix of second-order partial derivatives)
- Gradient, Divergent, and Curl
  - Gradient of a scalar function
  - Divergence of a vector field in 2D
  - Divergence of a vector field in 3D
  - Curl of a vector field in 2D
  - Curl of a vector field in 3D
- Multiple Integrals
  - Double Integrals (Cartesian Coordinates)
  - Double Integrals (Polar Coordinates)
  - Triple Integrals (Cartesian Coordinates)
  - Triple Integrals (Cylindrical Coordinates)
  - Triple Integrals (Spherical Coordinates)
  - Change of Order in Double Integrals
  - Surface Integrals
  - Volume Integrals
- Jacobian and Change of Variables
  - Jacobian Determinant (2D Cartesian to Polar Coordinates)
  - Jacobian Determinant (3D Cartesian to Cylindrical Coordinates)
  - Jacobian Determinant (3D Cartesian to Spherical Coordinates)
  - Transformation of Double Integrals using Jacobian
  - Transformation of Triple Integrals using Jacobian
  - Nonlinear Change of Variables in a Function
  
### Advanved Concepts
- Vector Calculus
  - Gradient of a Scalar Field
  - Divergence of a Vector Field
  - Curl of a Vector Field
  - Line Integral of a Scalar Function
  - Line Integral of a Vector Field
  - Conservative Vector Fields & Path Independence
  - Green’s Theorem (Circulation and Flux Forms)
  - Surface Integral of a Scalar Function
  - Surface Integral of a Vector Field
  - Stokes’ Theorem
  - Divergence Theorem (Gauss’ Theorem)
- Taylor Series and Approximation
  - Basic Taylor Series Expansion
  - Maclaurin Series
  - Taylor Approximation (Truncation of the Series)
  - Error in Taylor Approximation (Lagrange Remainder Term)
  - Taylor Series for Multivariable Functions
  - Approximating Square Roots or Logarithms
- Differential Equations
  - First-Order Ordinary Differential Equations (ODEs)
  - Second-Order Ordinary Differential Equations (ODEs)
  - Higher-Order Ordinary Differential Equations
  - Systems of Ordinary Differential Equations
  - Partial Differential Equations (PDEs) – First Order
  - Partial Differential Equations (PDEs) – Second Order
  - Boundary Value Problems (BVPs)
  - Laplace’s Equation and Harmonic Functions
  - Fourier Series & Transform Solutions for PDEs
  - Calculus of Variations (Euler-Lagrange Equation)

## Optimization
### Basic Concepts
- Objective Functions
- Convex and Non-Convex Functions
- Global vs. Local Minima/Maxima
- Gradient Descent (Basic)

### Constrained Optimization
- Linear Programming (LP)
- Quadratic Programming (QP)
- Lagrange Multipliers (Detailed)
- KKT Conditions (Karush-Kuhn-Tucker)

### Gradient-Based Variants
- Gradient Descents
  - SGD (Stochastic Gradient Descent)
  - Mini-Batch Gradient Descent
  - RMSProp (Root Mean Square Propagation)
- Momentum-Based Methods
  - Momentum
  - Nesterov Accelerated Gradient (NAG)
- Hybrid Variant (Gradient Descents & Momentum-based Methods)
  - Adam (Adaptive Moment Estimation)
- Newton’s Method and Quasi-Newton Methods

### Optimization for Machine Learning
- Stochastic Optimization
- Regularization Techniques
  - L2 (Ridge Regularization)
  - L1 (Lasso Regularization)
  - Elastic Net
  - Dropout
  - Early Stopping
- Hyperparameter Optimization
  - Grid Search
  - Random Search
  - Bayesian Optimization
  - Evolutionary Algorithms
    
### Advanced Optimization Topics
- Conjugate Gradient Method
- Trust-Region Methods
- Simulated Annealing
- Genetic Algorithms
- Coordinate Descent
  
### Optimization in Deep Learning
- Backpropagation and Gradient Flow
- Batch Normalization and Learning Rate Schedules
- Auto-Differentiation
