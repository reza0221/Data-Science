# Mathematics for Data Science
## Linear Algebra
### Basics
- Scalars
  - Basic Operations
  - Operations Related to Vector
  - Properties & Combinations
  - Statistical Context
- Vectors
  - Basic Vector Arithmetic
  - Vector Products
  - Vector Properties & Transformations
  - Vector-Based Geometric Interpretations
  - Vector Operations in Higher Dimensions
- Matrices
  - Matrix Addition
  - Matrix Subtraction
  - Scalar Multiplication
  - Matrix Multiplication (Dot Product)
  - Element-wise Multiplication
  - Matrix Transposition
  - Matrix Determinant Calculation
  - Matrix Inversion
  - Matrix Rank Calculation
  - Eigenvalues and Eigenvectors Calculation
- Tensors
  - Tensor Addition
  - Tensor Subtraction
  - Scalar Multiplication
  - Tensor Multiplication (Dot Product & Matrix Product)
  - Element-wise Multiplication
  - Tensor Transposition (Permutation of Axes)
  - Tensor Contraction (Reduction via Summation over Axes)
- Vector Spaces
  - Real Vector Space
  - Complex Vector Space
  - Euclidean Space (ℝⁿ Space)
  - Function Space
  - Polynomial Vector Space
  - Matrix Vector Space
  - Normed Vector Space
- Linear Combinations and Span
  - Basic Linear Combination
  - Checking if a Vector is in the Span
  - Span of Two Vectors
  - Span of a Set of Vectors
  - Basis and Dimension of a Span
- Linear Transformations
  - Scaling Transformation
  - Rotation Transformation
  - Reflection Transformation
  - Shear Transformation
  - Projection Transformation
  - Identity Transformation
  
### Advanced
- Dot Product and Cross Product
  - Dot Product
    - Two-Dimensional Vectors
    - Three-Dimensional Vectors
    - Dot Product Application in Projection
    - Angle Between Two Vectors Using Dot Product
    - Work Done by a Force
  - Cross Product
    - Three-Dimensional Vectors
    - Geometric Interpretation of Cross Product (Area of Parallelogram)
    - Torque Calculation Using Cross Product
    - Perpendicular Vectors and Cross Product
    - Unit Vector Normal to Two Vectors Using Cross Product
- Norms and Distances
  - L1 Norm (Manhattan Distance or Taxicab Norm)
  - L2 Norm (Euclidean Distance)
  - Lp Norm (Generalized Form of Norms)
  - Max Norm (Infinity Norm, Chebyshev Distance)
  - Frobenius Norm (Matrix Norm)
  - Cosine Similarity (Angular Distance in High Dimensions)
  - Mahalanobis Distance (Statistical Distance)
- Eigenvalues and Eigenvectors
  - Basic Eigenvalue and Eigenvector Calculation
  - Diagonalization Using Eigenvalues and Eigenvectors
  - Symmetric Matrix Eigenvalues
  - Singular Matrix Eigenvalue
  - Eigenvalues in Markov Chains
  - Eigenvalues and Eigenvectors in PCA
- Matrix Decomposition
  - LU Decomposition
    - General square matrix LU decomposition
    - LU decomposition with partial pivoting
  - QR Decomposition
    - QR decomposition for a full-rank square matrix
    - QR decomposition for a rectangular matrix
  - SV Decomposition
    - Full SVD for a square matrix
    - SVD for a rectangular matrix
    
## Calculus
### Basic Concepts
- Limits and Continuity
  - Basic Limit Calculation
  - One-Sided Limits
  - Limit at Infinity
  - Indeterminate Forms and L'Hôpital's Rule
  - Squeeze Theorem
  - Continuity at a Point
  - Discontinuities
  - Piecewise Function Continuity
- Differentiation
  - Basic Derivatives (Power Rule, Sum Rule, Constant Rule)
  - Product Rule
  - Quotient Rule
  - Chain Rule
  - Higher-Order Derivatives
  - Implicit Differentiation
  - Logarithmic Differentiation
  - Partial Derivatives (for multivariable functions)
  - Directional Derivatives
  - Gradient and Hessian Matrix
- Integration
  - Indefinite Integrals (Antiderivatives)
  - Definite Integrals (With Limits)
  - Integration by Substitution (u-Substitution)
  - Integration by Parts
  - Trigonometric Integrals (Using Trigonometric Identities)
  - Improper Integrals (Infinite Limits or Discontinuous Functions)
  - Double Integrals (Multivariable Integration)
  - Triple Integrals (Extending to 3D Volume Computations)
- Fundamental Theorem of Calculus (FTC)
  - First Fundamental Theorem of Calculus (FTC-1)
  - Second Fundamental Theorem of Calculus (FTC-2)
  
### Multivariable Calculus
- Partial Derivatives
  - First-Order Partial Derivative (∂f/∂x and ∂f/∂y)
  - Second-Order Partial Derivative (∂²f/∂x², ∂²f/∂y², and mixed derivatives ∂²f/∂x∂y)
  - Gradient (∇f) (Vector of first-order partial derivatives)
  - Directional Derivative (Rate of change in a given direction)
  - Jacobian Matrix (Matrix of first-order partial derivatives for multivariable functions)
  - Hessian Matrix (Square matrix of second-order partial derivatives)
- Gradient, Divergent, and Curl
  - Gradient of a scalar function
  - Divergence of a vector field in 2D
  - Divergence of a vector field in 3D
  - Curl of a vector field in 2D
  - Curl of a vector field in 3D
- Multiple Integrals
  - Double Integrals (Cartesian Coordinates)
  - Double Integrals (Polar Coordinates)
  - Triple Integrals (Cartesian Coordinates)
  - Triple Integrals (Cylindrical Coordinates)
  - Triple Integrals (Spherical Coordinates)
  - Change of Order in Double Integrals
  - Surface Integrals
  - Volume Integrals
- Jacobian and Change of Variables
  - Jacobian Determinant (2D Cartesian to Polar Coordinates)
  - Jacobian Determinant (3D Cartesian to Cylindrical Coordinates)
  - Jacobian Determinant (3D Cartesian to Spherical Coordinates)
  - Transformation of Double Integrals using Jacobian
  - Transformation of Triple Integrals using Jacobian
  - Nonlinear Change of Variables in a Function
  
### Advanved Concepts
- Vector Calculus
  - Gradient of a Scalar Field
  - Divergence of a Vector Field
  - Curl of a Vector Field
  - Line Integral of a Scalar Function
  - Line Integral of a Vector Field
  - Conservative Vector Fields & Path Independence
  - Green’s Theorem (Circulation and Flux Forms)
  - Surface Integral of a Scalar Function
  - Surface Integral of a Vector Field
  - Stokes’ Theorem
  - Divergence Theorem (Gauss’ Theorem)
- Taylor Series and Approximation
  - Basic Taylor Series Expansion
  - Maclaurin Series
  - Taylor Approximation (Truncation of the Series)
  - Error in Taylor Approximation (Lagrange Remainder Term)
  - Taylor Series for Multivariable Functions
  - Approximating Square Roots or Logarithms
- Differential Equations
  - First-Order Ordinary Differential Equations (ODEs)
  - Second-Order Ordinary Differential Equations (ODEs)
  - Higher-Order Ordinary Differential Equations
  - Systems of Ordinary Differential Equations
  - Partial Differential Equations (PDEs) – First Order
  - Partial Differential Equations (PDEs) – Second Order
  - Boundary Value Problems (BVPs)
  - Laplace’s Equation and Harmonic Functions
  - Fourier Series & Transform Solutions for PDEs
  - Calculus of Variations (Euler-Lagrange Equation)

## Optimization
### Foundational Optimization Theory
- Convex vs. Non-Convex Optimization
  - Convex sets, functions, duality (Lagrange multipliers, Fenchel conjugates).
  - Non-convex landscapes: saddle points, local minima, convergence guarantees.
- Constrained Optimization
  - Equality constraints (Lagrange multipliers).
  - Inequality constraints (KKT conditions, duality gaps).
  - Penalty methods, barrier functions (interior-point methods).
- Global vs. Local Optimization
  - Complexity theory (NP-hardness of non-convex problems).
  - Basin hopping, multi-start methods.
- Linear & Quadratic Programming
  - Simplex method, duality in LP/QP.
  - KKT conditions applied to LP/QP.

### Gradient-Based Optimization
- First-Order Methods
  - Gradient Descent (GD), Stochastic GD (SGD), Mini-Batch GD.
  - Convergence analysis: Lipschitz continuity, Robbins-Monro conditions.
  - Online optimization (regret bounds).
- Momentum & Acceleration
  - Polyak’s Heavy Ball, Nesterov Accelerated Gradient (NAG).
  - Theoretical acceleration rates (O(1/k²) vs. O(1/k)).
- Adaptive Methods
  - RMSProp, Adam, AMSGrad.
  - Convergence issues in non-convex settings (e.g., Adam’s divergence).
- Second-Order Methods
  - Newton’s Method, Quasi-Newton (BFGS, L-BFGS).
  - Hessian-free optimization, K-FAC for neural networks.

### Constrained & Regularized Optimization
- Explicit Regularization
  - L1/L2 (proximal operators, subgradient methods).
  - Elastic Net (group sparsity).
  - Duality in LASSO (link to KKT conditions).
- Implicit Regularization
  - SGD-induced regularization (e.g., margin maximization in SVMs).
  - Early stopping as iterative regularization.
- Distributed Optimization
  - ADMM, consensus optimization, federated learning frameworks.
- Proximal Algorithms
  - Proximal gradient descent, splitting methods (e.g., Douglas-Rachford).
    
### Global & Non-Convex Optimization
- Heuristic Methods
  - Simulated Annealing (Metropolis-Hastings criterion).
  - Genetic Algorithms (crossover, mutation, selection operators).
- Bayesian Optimization
  - Gaussian processes, acquisition functions (EI, UCB)
- Non-Convex Convergence
  - Escaping saddle points (noise-injection theory), Lyapunov analysis.
  
### Optimization in Machine Learning
- Hyperparameter Optimization
  - Grid/Random Search (combinatorial vs. continuous space).
  - Bayesian Optimization (theoretical regret bounds).
- Stochastic Optimization
  - Variance reduction techniques (SAGA, SVRG).
  - Mini-batch tradeoffs (bias-variance in gradients).
- Deep Learning Optimization
  - Backpropagation (computation graphs, Jacobian-vector products).
  - Batch Normalization (smoothing the loss landscape).
  - Gradient clipping (preventing explosion), initialization (Xavier/He).
  - Auto-differentiation (forward vs. reverse mode, computational complexity).

### Advanced Numerical Methods
- Trust-Region Methods
  - Trust-region vs. line search, convergence proofs.
- Coordinate Descent
  - Cyclic vs. stochastic selection, applications to LASSO.
- Conjugate Gradient
  - Krylov subspace methods, convergence in quadratic forms.
